{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- 🤝 Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TLDR version - made possible by the makers of LangSmith\n",
        "\n",
        "The key concepts of this Lab are clearly captured by some essential LangSmith capabilities\n",
        "- [Evaluation Dataset](https://smith.langchain.com/public/fb3e23b9-31c5-49e0-9b88-3f771a75fa36/d)\n",
        "    - Experiment Name:  drill down on the experiment record to view details on the sample input, reference output, and evaluation results for \"example\" requests\n",
        "- [Example Trace](https://smith.langchain.com/public/725291df-8a70-40d3-b26a-1b224fcd73c4/r)\n",
        "    - being able to view the functions and tools that an LLM agent has access to in a LangGraph state machine instance is a powerful learning experience\n",
        "    - also essential to be review the tool description that was provided to the LLM.  If your state machine isn't behaving as expected here's a great place to start!\n",
        "    - also provides visibility to an LLM's decisions on what tool (if any) should be called next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lessons Learned:\n",
        "\n",
        "1. **The Power of Graphical Thinking in AI Development:**\n",
        "   - 🌐 **Graphical Paradigm Shift:** LangGraph's approach of translating flowcharts into functional code reveals a powerful paradigm shift in AI development.\n",
        "   - 🔗 **Bridging Design and Implementation:** This \"visual-to-functional\" methodology bridges the gap between conceptual design and implementation, potentially democratizing complex AI system development.\n",
        "   - 🌟 **Democratizing AI Design:** Future AI tools might evolve towards more visual, intuitive interfaces, allowing a broader range of professionals (not just programmers) to contribute to AI system design. This could lead to more diverse and innovative AI applications across various industries.\n",
        "\n",
        "2. **The Renaissance of State Machines in Modern AI:**\n",
        "   - 🏛️ **Classical Meets Cutting-Edge:** The emphasis on state machines in LangGraph highlights a fascinating convergence of classical computer science concepts with cutting-edge AI. It demonstrates that as AI systems become more complex, we're circling back to fundamental computational ideas to manage this complexity.\n",
        "   - 🛠️ **Back to Basics:** This \"back to basics\" approach, combined with advanced language models, creates a powerful hybrid that balances the flexibility of AI with the reliability of traditional software engineering. It suggests that the future of AI might not be about abandoning established computer science principles, but rather about reimagining and repurposing them for the AI era.\n",
        "\n",
        "3. **The Emergence of AI Introspection through Self-Evaluation:**\n",
        "   - 🔍 **AI Self-Critique:** The implementation of the helpfulness check in the LangGraph system points to an intriguing development: AI systems that can critique and improve their own outputs.\n",
        "   - 🧠 **Artificial Introspection:** This self-evaluation capability hints at a form of artificial introspection, where the AI doesn't just produce results but also assesses their quality. As this concept evolves, we might see AI systems that not only perform tasks but also explain their reasoning, acknowledge their limitations, and actively work to improve their performance.\n",
        "   - 🤝 **Trust and Transparency:** This could lead to more transparent, trustworthy, and continuously improving AI systems, potentially addressing some of the current concerns about AI reliability and bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ongoing Learning - key areas where mastery is essential\n",
        "\n",
        "1. **Multi-Agent Dynamics and Coordination:**\n",
        "   - 🕹️ **Design and Implementation:** design and implement robust multi-agent systems using LangGraph.\n",
        "   - 📡 **Inter-System Communication:** Explore strategies for inter-system communication and sharing of state information.\n",
        "   - ⚔️ **Conflict Resolution:** Understand methods to resolve conflicts or contradictions between agents.\n",
        "   - 📈 **Performance Evaluation:** Evaluate the collective performance of a multi-agent system.\n",
        "\n",
        "2. **Long-Term Memory and Continuous Learning:**\n",
        "   - 🧠 **Implement Long-Term Memory:** do a deeper dive on implementing long-term memory in LangGraph agents, including checkpointing and persistence.\n",
        "   - 📚 **Continuous Learning:** Study techniques for continuous learning and updating LLM processes based on interaction history.\n",
        "   - 🧹 **Data Management:** Learn methods to balance between retaining useful information and avoiding outdated or irrelevant data.\n",
        "   - 🏆 **Evaluation Metrics:** Develop evaluation metrics for assessing an agent's ability to learn and improve over time.\n",
        "\n",
        "3. **Robust and Comprehensive Evaluation Frameworks:**\n",
        "   - 📊 **Holistic Evaluation Metrics:** Design holistic evaluation metrics that are fit for purpose and balance trade-offs.\n",
        "   - 🎯 **Response Assessment:** Assess not just the correctness of responses, but also their relevance, coherence, and ethical implications over time as part of the overall LLMOps strategy.\n",
        "   - 👍 **Human Feedback Integration:** Integrate human feedback more effectively into the evaluation process.\n",
        "   - 🛠️ **Automated Generation of Evaluation data:** Evolve techniques learned from RAGAS and LangSmith to improve quality and diversity of generated test cases.\n",
        "   - 🔄 **Complex Evaluations:** Continue to use LangSmith, RAGAS, and WandB for more complex, multi-stage evaluations that mirror real-world use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(), ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  description of the tool generated by convert_to_openai_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'duckduckgo_search',\n",
              "  'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'arxiv',\n",
              "  'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        +-----------+           \n",
            "        | __start__ |           \n",
            "        +-----------+           \n",
            "               *                \n",
            "               *                \n",
            "               *                \n",
            "          +-------+             \n",
            "          | agent |             \n",
            "          +-------+             \n",
            "          .        ..           \n",
            "        ..           ..         \n",
            "       .               .        \n",
            "+--------+         +---------+  \n",
            "| action |         | __end__ |  \n",
            "+--------+         +---------+  \n"
          ]
        }
      ],
      "source": [
        "app.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### HEY BATMAN!!!  WHERE'S YOUR TOOL BELT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        app.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install pygraphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Image(app.get_graph().draw_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?  \n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER: TLDR answer is No and Yes\n",
        "\n",
        "1. Is there any specific limit to how many times we can cycle?\n",
        "- There is no inherent limit to the number of cycles within the LangGraph system.\n",
        "- The system's design allows for potentially infinite loops unless explicitly constrained. This flexibility is part of what makes LangGraph suitable for complex agent workflows that may require multiple cycles to reach a conclusion.\n",
        "\n",
        "2. How could we impose a limit on the number of cycles?\n",
        "\n",
        "The LangGraph system allows for implementing cycle limits through conditions in the state management. Here are two ways to impose such limits:\n",
        "\n",
        "a) You can use a condition within your function to check the number of cycles or messages. For example, in the check_helpfulness function, you can implement a cycle limit by checking the length of the state object:\n",
        "\n",
        "``` python\n",
        "def check_helpfulness(state):\n",
        "    if len(state[\"messages\"]) > 10:\n",
        "        return \"END\"\n",
        "    # ... rest of the function\n",
        "``` \n",
        "\n",
        "This approach checks the number of messages and terminates the cycle if it exceeds 10.\n",
        "\n",
        "b) By adding a condition that checks the number of iterations, you can force the agent to exit if a certain threshold is reached. This ensures that the agent will eventually terminate its process even if it hasn't reached a satisfactory conclusion through its normal decision-making process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of Large Language Models (LLMs) to enhance their performance by combining retrieval-based methods with generative models. The core idea is to augment the generative capabilities of LLMs with relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps in generating more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component searches a large corpus to find relevant documents or passages based on the input query.\n",
            "2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Retrieval Step**: The retriever searches a pre-indexed corpus to find the most relevant documents or passages.\n",
            "3. **Generation Step**: The generator uses the retrieved documents along with the original query to generate a coherent and contextually relevant response.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and detailed responses.\n",
            "- **Up-to-date Information**: It can incorporate the latest information from the corpus, making it useful for answering questions about recent events or specialized topics.\n",
            "- **Contextual Relevance**: The retrieval step ensures that the generated response is grounded in relevant context, improving the overall quality of the output.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced in a research paper by Facebook AI (now Meta AI) in 2020. The paper, titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" was presented at the 2020 Conference on Neural Information Processing Systems (NeurIPS). The introduction of RAG marked a significant advancement in the field of natural language processing, particularly for tasks that require integrating external knowledge with generative models.\n",
            "\n",
            "Would you like more detailed information or specific papers on RAG?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-06-12\n",
            "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "Summary: There are various methods for adapting LLMs to different domains. The most\n",
            "common methods are prompting, finetuning, and RAG. In this w\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\n",
            "\n",
            "Agent Response: ### QLoRA in Machine Learning\n",
            "\n",
            "**QLoRA** stands for \"Quantized Low-Rank Adaptation.\" It is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key innovations of QLoRA include:\n",
            "\n",
            "1. **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\n",
            "2. **Double Quantization**: Reduces the average memory footprint by quantizing the quantization constants.\n",
            "3. **Paged Optimizers**: Manage memory spikes during training.\n",
            "\n",
            "QLoRA allows for the finetuning of large models, such as a 65 billion parameter model, on a single 48GB GPU. The approach backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The resulting models, such as the Guanaco family, have shown to outperform previous openly released models on benchmarks like Vicuna, achieving 99.3% of the performance level of ChatGPT with significantly reduced resource requirements.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Published**: 2023-05-23\n",
            "   - **Summary**: This paper introduces QLoRA and its innovations, providing a detailed analysis of instruction following and chatbot performance across various datasets and model scales.\n",
            "\n",
            "2. **Title**: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Published**: 2024-05-27\n",
            "   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention techniques.\n",
            "\n",
            "3. **Title**: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "   - **Published**: 2024-06-12\n",
            "   - **Summary**: This paper explores the adaptation of LLMs to different domains using QLoRA, focusing on fact memorization and style imitation.\n",
            "\n",
            "### Bio of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a researcher whose work focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His research involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### 🏗️ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  LangSmith has the best summary of this, capturing the interaction between the agent and action nodes.\n",
        "\n",
        "- review the detail here:  https://smith.langchain.com/public/2103299e-0a95-495d-81ff-30b9ad6e2df2/r\n",
        "\n",
        "Following screenshots focus on the calls to the LLM throughout the lifecycle of the instance of the LangGraph call.\n",
        "\n",
        "1. initial request to LLM - first request for tool action\n",
        "![Screenshot 2024-07-09 221433.png](./images/Screenshot-2024-07-09-221433.png)\n",
        "\n",
        "2. second LLM request - request for followup tool action\n",
        "![Screenshot 2024-07-09 221405.png](./images/Screenshot-2024-07-09-221405.png)\n",
        "\n",
        "3. third LLM request - no function call in response, and provides response to initial query\n",
        "![Screenshot 2024-07-09 221457.png](./images/Screenshot-2024-07-09-221457.png)\n",
        "\n",
        "4. the END\n",
        "![Screenshot 2024-07-09 221811.png](./images/Screenshot-2024-07-09-221811.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide more context and relevant information.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers because it has access to additional information from the retrieval step.\\n\\nRAG is particularly useful in scenarios where the model needs to generate responses based on a large and diverse set of information, such as in open-domain question answering, customer support, and knowledge-based systems. By leveraging both retrieval and generation, RAG can provide more accurate and informative responses compared to using either approach alone.\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### 🏗️ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  the questions and answers are associated by their position in two separate lists.\n",
        "\n",
        "The association is implicit, based on the index. The first question corresponds to the first answer, the second question to the second answer, and so on.\n",
        "This method is problematic for several reasons:\n",
        "\n",
        "1. It's error-prone: If the lists get out of sync (e.g., a question is added without a corresponding answer), it could lead to incorrect evaluations.\n",
        "2. It's not self-documenting: Someone reading the code later might not immediately understand how questions and answers are related.\n",
        "3. It's inflexible: Adding or removing questions/answers in the middle of the lists could disrupt all subsequent associations.\n",
        "\n",
        "A more robust approach would be to use a dictionary or a list of tuples to explicitly pair questions with their corresponding answers.\n",
        "\n",
        "NOTE:  a LangSmith dataset has been created with the questions and answers about the QLoRA paper.  This at least provides a means of validation\n",
        "\n",
        "- https://smith.langchain.com/public/fb3e23b9-31c5-49e0-9b88-3f771a75fa36/d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  \n",
        "\n",
        "The current metric is a simple \"must mention\" check, some ways to improve this metric or address its gaps include:\n",
        "\n",
        "- Case sensitivity: The current implementation is case-sensitive. You could make it case-insensitive for more flexible matching.\n",
        "- Partial matching: It currently checks for exact substrings. You could implement fuzzy matching or stemming to account for variations in word forms.\n",
        "- Contextual understanding: The current method doesn't consider the context in which words appear. You could implement a more sophisticated NLP approach to ensure the required phrases are used in the correct context.\n",
        "- Weighting: All required phrases are treated equally. You could implement a weighting system where some phrases are more important than others.\n",
        "- Negation handling: The current method doesn't account for negations. A response containing \"not NF4\" would still be considered correct if \"NF4\" is a required phrase.\n",
        "- Synonyms: It doesn't recognize synonyms or paraphrases. You could implement a synonym recognition system.\n",
        "- Scoring granularity: The current scoring is binary (all or nothing). You could implement a more granular scoring system, perhaps giving partial credit for mentioning some but not all required phrases.\n",
        "\n",
        "\n",
        "reference:  https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - fcc59b0c' at:\n",
            "https://smith.langchain.com/o/a7c25b04-2d2b-5cac-90ab-f8d1cb327556/datasets/84470f1d-1a4d-4e94-b97d-d2f16cb9c43a/compare?selectedSessions=a7681832-e5be-471d-b523-f2fb84bfdf17\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - d2ba2462 at:\n",
            "https://smith.langchain.com/o/a7c25b04-2d2b-5cac-90ab-f8d1cb327556/datasets/84470f1d-1a4d-4e94-b97d-d2f16cb9c43a\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - fcc59b0c',\n",
              " 'results': {'5d3fec3a-d272-4884-8d0b-52d8f1e7c591': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question, stating that the AdamW optimizer is used in QLoRA. \\n\\nIn addition to answering the question, the submission also provides additional information about the AdamW optimizer, explaining that it is a variant of the Adam optimizer that includes weight decay. This information is insightful as it provides context about why this particular optimizer might be used in QLoRA.\\n\\nThe submission also mentions that the AdamW optimizer is well-suited for training large-scale language models and is commonly used in various fine-tuning tasks. This information is appropriate as it relates to the use of the optimizer in QLoRA, a method used for training language models.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('72c52557-3ab4-4bef-b970-e722bfee2d2d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context does not provide any information about the optimizer used in QLoRA. However, the student's answer states that QLoRA uses the AdamW optimizer. Without any conflicting information in the context, we can't definitively say that the student's answer is incorrect. However, we also can't confirm that it's correct based on the provided context.\\nGRADE: CANNOT BE DETERMINED\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8d3a9f52-e087-4b52-a63b-e9b089c5869c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('41d03c11-9246-44ec-8312-8a328a0d0d49'), target_run_id=None)],\n",
              "   'execution_time': 2.570913,\n",
              "   'run_id': '3f2d8efe-d14f-407e-82aa-68215900163c',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay, which helps in regularizing the model and preventing overfitting. This optimizer is well-suited for training large-scale language models and is commonly used in various fine-tuning tasks.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  'a90bf1f4-feb1-4bae-bf2b-78e2b06dead0': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and concise answer to the question asked. It not only names the data type created in the QLoRA paper, but also provides additional information about its purpose and usage. This additional context makes the answer more helpful and insightful for the reader. \\n\\nThe submission is also appropriate as it directly addresses the question and does not include any irrelevant or inappropriate content. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a7ded05d-412b-4c71-86cb-44b6260d0879'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions that the data type created in the QLoRA paper is 'NF4' or 'NormalFloat'. The student's answer also mentions that the data type created in the QLoRA paper is 'NF4' or 'NormalFloat'. Therefore, the student's answer matches the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a933b07c-fcfd-4920-9888-4a12f932bd10'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6b8faec7-b6a7-4a22-9836-1ee7831c1886'), target_run_id=None)],\n",
              "   'execution_time': 2.901515,\n",
              "   'run_id': 'bbabdd71-11c0-43e7-bbc5-79ea9c880322',\n",
              "   'output': 'The QLoRA (Quantized Low-Rank Adaptation) paper introduced a new data type called `NF4` (4-bit NormalFloat). This data type is designed to be used in the context of quantized low-rank adaptation of large language models, allowing for efficient storage and computation while maintaining model performance.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '529625af-6110-4a89-96c0-69ea8e956600': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of what a Retrieval Augmented Generation system is, breaking down its components and explaining how they work together. This is helpful for someone trying to understand the concept.\\n\\n2. The submission also provides a step-by-step breakdown of how a RAG system works, from receiving an input query to generating a response. This is insightful and helps to further clarify the concept.\\n\\n3. The submission goes beyond just explaining what a RAG system is and also discusses its benefits and potential applications. This is appropriate and adds value to the explanation.\\n\\n4. The language used in the submission is clear and easy to understand, making the information accessible to a wide range of readers.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cbf00b5e-2128-48a7-a17b-1c69f6e9a120'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is detailed and comprehensive. It accurately describes what a Retrieval-Augmented Generation (RAG) system is, how it works, its benefits, and its applications. The student explains the two main components of a RAG system: the retrieval-based component and the generation-based component. They also provide a step-by-step breakdown of how a RAG system works, from receiving an input query to generating a response. The student also highlights the benefits of using a RAG system, such as improved relevance, knowledge integration, and contextual understanding. Lastly, the student mentions some applications of RAG systems, including question answering, customer support, and content creation. Therefore, the student's answer is factually accurate and aligns with the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('02b7848b-b11e-4783-aa28-5730256db2d3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ed972072-7ed6-44b6-889f-8f97f17352ed'), target_run_id=None)],\n",
              "   'execution_time': 5.752158,\n",
              "   'run_id': '58fa1d98-b98f-4dc9-b1dc-01d6d05c6e2d',\n",
              "   'output': 'Retrieval-Augmented Generation (RAG) is a type of natural language processing (NLP) system that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here’s a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**:\\n   - This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query.\\n   - It uses techniques such as TF-IDF, BM25, or more advanced neural retrieval models to find the most relevant documents.\\n\\n2. **Generation-Based Component**:\\n   - This part of the system generates text based on the retrieved documents and the input query.\\n   - It typically uses advanced language models like GPT-3, BERT, or other transformer-based models to generate coherent and contextually appropriate responses.\\n\\n### How RAG Works:\\n1. **Input Query**: The system receives an input query from the user.\\n2. **Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Augmentation**: The retrieved documents are then used to augment the input query, providing additional context and information.\\n4. **Generation**: The generation component uses the augmented input to generate a response that is informed by the retrieved documents.\\n\\n### Benefits of RAG:\\n- **Improved Relevance**: By leveraging external documents, the system can provide more accurate and relevant responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from a large corpus, making it more knowledgeable.\\n- **Contextual Understanding**: The generation component can produce more contextually appropriate and coherent responses.\\n\\n### Applications:\\n- **Question Answering**: Providing detailed and accurate answers to user queries.\\n- **Customer Support**: Assisting in customer service by retrieving and generating relevant responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, and other content by providing relevant information and context.\\n\\nOverall, RAG systems aim to combine the strengths of both retrieval and generation to create more powerful and effective NLP applications.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '073fe52a-725a-4347-809d-a4d243b205b5': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides an answer to the question asked, which is about the authorship of the QLoRA paper. The response indicates that the paper was developed by members of the University of Washington\\'s UW NLP group. This information is helpful and appropriate as it answers the question directly and provides additional context about the group that authored the paper. \\n\\nThe submission does not provide any misleading or incorrect information, and it is insightful as it provides more than just a simple answer to the question. It also includes the full title of the paper, which adds to its helpfulness.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9a20653c-4000-48cb-874d-a09e5f86bcab'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The context provided states that the authors of the QLoRA paper are 'Tim' and 'Dettmers'. The student's answer does not mention these names, instead attributing the paper to members of the University of Washington's UW NLP group. Without any additional information linking 'Tim' and 'Dettmers' to this group, the student's answer cannot be verified as correct based on the provided context.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5d4905a9-1bf1-4795-9f7e-1964caaba5ca'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('94b9f104-cbc7-471b-a119-e43d992e66f7'), target_run_id=None)],\n",
              "   'execution_time': 4.962142,\n",
              "   'run_id': '5a9d91f3-4f83-4a70-ae17-207715f96252',\n",
              "   'output': 'The QLoRA paper, titled \"QLoRA: Efficient Finetuning of Quantized LLMs,\" was developed by members of the University of Washington\\'s UW NLP group.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '7b808a24-bee4-44f0-969a-56ecc5619253': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed answer to the question, listing the three most popular deep learning frameworks as of 2023. It not only names the frameworks but also provides additional information about each one, such as who developed them, their key features, and why they are popular. This information is relevant and insightful, as it gives the reader a better understanding of each framework and its use cases. \\n\\nThe submission is also appropriate, as it directly answers the question and stays on topic. It does not include any irrelevant or inappropriate content. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('be81d6f0-057d-47b7-b00a-42a2dfe1e497'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer lists TensorFlow and PyTorch as popular deep learning frameworks, which matches the context provided. The student also mentions Keras, which is not mentioned in the context, but this does not contradict the context as the context does not state that only TensorFlow and PyTorch are popular. The student's answer is factually accurate and does not conflict with the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('494b86c3-6a7d-4f17-afc1-9e1c026b320b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a64a7f02-3739-4eb9-aa7d-9d9fc111b2b5'), target_run_id=None)],\n",
              "   'execution_time': 5.277572,\n",
              "   'run_id': 'f8e302fd-d803-4948-a7e0-467100ef2e9f',\n",
              "   'output': \"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It has a strong community and is used by major corporations like Airbnb and Intel.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is highly favored in the research community for its ease of use, flexibility, and strong support for GPU acceleration. It is known for its dynamic computation graph and is widely used for state-of-the-art (SOTA) models.\\n\\n3. **Keras**: Initially developed by François Chollet, Keras is a high-level neural network API written in Python. It is user-friendly and runs on top of TensorFlow, Theano, and CNTK, making it a popular choice for beginners and rapid prototyping.\\n\\nThese frameworks are highly regarded for their capabilities, community support, and extensive use in both industry and research.\",\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  'c0a69a56-6806-4b73-bc2b-36b8fb7b4a44': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission needs to be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the improvements that the LoRA system brings. It lists seven significant improvements, each with a clear and concise explanation. This makes the submission helpful as it provides the information that was asked for in the input.\\n\\nThe submission is also insightful. It not only lists the improvements but also explains why these improvements are significant. For example, it explains how parameter efficiency leads to faster training and how improved generalization can lead to better performance on unseen data. This gives the reader a deeper understanding of the benefits of the LoRA system.\\n\\nFinally, the submission is appropriate. It directly answers the question asked in the input and stays on topic throughout. It uses technical language that is suitable for the subject matter and provides a comprehensive answer to the question.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3c060ee2-6aa5-4ac2-b264-01688605aee1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context provided is 'reduce' and 'parameters'. The student's answer mentions that LoRA significantly reduces the number of trainable parameters required for fine-tuning, which aligns with the context provided. The student's answer also provides additional information about the benefits of LoRA, but none of this information contradicts the context or the question. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('61a34a81-d7fd-4931-a6dd-9a40a549c15e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c18cd408-e092-420d-aa76-ece2cdcab311'), target_run_id=None)],\n",
              "   'execution_time': 8.762561,\n",
              "   'run_id': 'e1f4499d-c232-4943-8788-7e1eb4ffe63c',\n",
              "   'output': 'LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. Here are some significant improvements that LoRA brings:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of a large pre-trained model, LoRA introduces a small number of additional parameters that capture the necessary adaptations. This makes the fine-tuning process more efficient and less resource-intensive.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This is particularly beneficial when working with very large models that would otherwise require substantial computational resources.\\n\\n3. **Faster Training**: With fewer parameters to update, the training process becomes faster. This allows for quicker iterations and experimentation, which is valuable in research and development settings.\\n\\n4. **Improved Generalization**: LoRA can help in achieving better generalization by focusing on low-rank adaptations. This can lead to models that perform better on unseen data, as the fine-tuning process is more targeted and less prone to overfitting.\\n\\n5. **Modularity**: LoRA allows for modular adaptations, meaning that different low-rank adaptations can be combined or swapped in and out as needed. This modularity can be useful for creating specialized models for different tasks without needing to retrain the entire model from scratch.\\n\\n6. **Scalability**: LoRA is scalable to very large models, making it a practical approach for fine-tuning state-of-the-art language models that have billions of parameters.\\n\\n7. **Cost-Effectiveness**: By reducing the computational and memory requirements, LoRA makes the fine-tuning process more cost-effective. This is particularly important for organizations with limited resources.\\n\\nOverall, LoRA provides a more efficient and effective way to fine-tune large language models, making it a valuable technique in the field of machine learning.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### 🏗️ Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  serves as a quality check within the LangGraph system, ensuring that the agent's responses meet a certain standard of helpfulness.\n",
        "\n",
        "It allows the system to:\n",
        "\n",
        "- Self-evaluate its performance\n",
        "- Potentially retry or refine responses that are deemed unhelpful\n",
        "- Terminate the conversation when a satisfactory response is provided or when the cycle limit is reached\n",
        "\n",
        "Here's a breakdown of its functionality:\n",
        "\n",
        "1. Cycle Limit:\n",
        "\n",
        "- It first checks if the conversation has exceeded 10 messages. If so, it returns \"END\" to prevent infinite loops.\n",
        "\n",
        "\n",
        "2. Prompt Creation:\n",
        "\n",
        "- It creates a prompt template that asks an AI model to determine if the final response is extremely helpful or not.\n",
        "\n",
        "\n",
        "3. Model Invocation:\n",
        "\n",
        "- It uses a GPT-4o model to evaluate the helpfulness of the response based on the initial query and final response.\n",
        "\n",
        "\n",
        "4. Helpfulness Determination:\n",
        "\n",
        "- If the model's response contains 'Y', it considers the response helpful, prints \"Helpful!\", and returns \"end\".\n",
        "- If not, it prints \"Not helpful!\" and returns \"continue\".\n",
        "\n",
        "\n",
        "5. Flow Control:\n",
        "\n",
        "- The return values \"end\" and \"continue\" are used to control the flow in the LangGraph, determining whether to terminate the conversation or continue processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### 🏗️ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### add nodes to the graph\n",
        "\n",
        "- importance of the dummy node in processing state\n",
        "- better terms could potentially be used to describe the purpose of the node and the function (they are confirmation of helpfulness check passing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### set entry point for the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### add conditional edges to the graph that define conditions for state transtions between nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### add edge between action and agent nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### make the graph executable.  Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         +-----------+              \n",
            "         | __start__ |              \n",
            "         +-----------+              \n",
            "                *                   \n",
            "                *                   \n",
            "                *                   \n",
            "           +-------+                \n",
            "           | agent |                \n",
            "           +-------+.               \n",
            "          ..         ..             \n",
            "        ..             ..           \n",
            "       .                 .          \n",
            "+--------+         +-------------+  \n",
            "| action |         | passthrough |  \n",
            "+--------+         +-------------+  \n",
            "                          .         \n",
            "                          .         \n",
            "                          .         \n",
            "                     +---------+    \n",
            "                     | __end__ |    \n",
            "                     +---------+    \n"
          ]
        }
      ],
      "source": [
        "agent_with_helpfulness_check.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### Run the LangGraph graph given a sample human message -- and print out the messages..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA, or Low-Rank Adaptation, is a technique used in machine learning to adapt pre-trained models to new tasks with minimal computational resources. The idea is to decompose the weight matrices of neural networks into low-rank matrices, which can be fine-tuned more efficiently. This approach is particularly useful in scenarios where computational resources are limited, as it reduces the number of parameters that need to be updated during training.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher known for his work in the field of machine learning, particularly in the areas of efficient training and inference of large-scale neural networks. He has contributed to the development of techniques that make it feasible to train large models on consumer-grade hardware. His research often focuses on optimizing the computational aspects of deep learning to make it more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism in neural networks that allows the model to focus on specific parts of the input data when making predictions. It was introduced to improve the performance of models in tasks like machine translation, where understanding the context of each word is crucial. The attention mechanism assigns different weights to different parts of the input, enabling the model to \"attend\" to the most relevant information. This concept is a cornerstone of Transformer models, which have become the state-of-the-art in many natural language processing tasks.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\", \"what is string?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP). It involves crafting specific inputs (prompts) to guide the behavior and output of AI models, particularly large language models like GPT-3, GPT-4, and others. The goal is to elicit desired responses from the model by carefully designing the input text.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Input Design**: Creating prompts that are clear, specific, and tailored to the task at hand.\n",
            "2. **Iterative Refinement**: Continuously refining prompts based on the model's responses to improve accuracy and relevance.\n",
            "3. **Contextual Awareness**: Ensuring that the prompt provides enough context for the model to understand the task.\n",
            "4. **Task-Specific Prompts**: Designing prompts that are specific to the task, whether it's text generation, question answering, summarization, translation, etc.\n",
            "\n",
            "### Historical Context:\n",
            "Prompt engineering became particularly prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to perform a wide range of tasks based on the input prompt highlighted the importance of prompt design. Researchers and practitioners quickly realized that the quality of the prompt could significantly influence the model's performance.\n",
            "\n",
            "### Timeline:\n",
            "- **Pre-2020**: Early NLP models required extensive training on specific tasks, and the concept of prompt engineering was not widely recognized.\n",
            "- **June 2020**: OpenAI released GPT-3, a powerful language model capable of performing various tasks based on prompts. This release brought prompt engineering into the spotlight.\n",
            "- **Post-2020**: The field of prompt engineering grew rapidly as more researchers and developers began experimenting with different prompt designs to optimize model performance.\n",
            "\n",
            "In summary, prompt engineering is a crucial technique in the use of modern AI language models, gaining significant attention and development following the release of GPT-3 in 2020.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a framework that combines the strengths of retrieval-based and generation-based models to improve the performance of natural language processing tasks, particularly in the context of question answering and information retrieval.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates a coherent and contextually appropriate response using the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: A user inputs a query.\n",
            "2. **Document Retrieval**: The retriever fetches relevant documents or passages from a pre-indexed corpus.\n",
            "3. **Response Generation**: The generator uses the retrieved documents to generate a response that is both relevant and contextually appropriate.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external documents, RAG can provide more accurate and contextually relevant answers.\n",
            "- **Scalability**: It can handle large corpora of documents, making it suitable for applications requiring extensive knowledge bases.\n",
            "- **Flexibility**: It can be fine-tuned for specific domains or tasks, enhancing its versatility.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, such as open-domain question answering and fact verification.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Aspects of Fine-Tuning:\n",
            "1. **Pre-trained Model**: A model that has already been trained on a large and diverse dataset.\n",
            "2. **Target Task**: A specific task or dataset that the model needs to adapt to.\n",
            "3. **Transfer Learning**: The concept of transferring knowledge from one domain (the pre-trained model) to another (the target task).\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
            "- **Performance**: Often achieves better performance on specific tasks due to the pre-trained model's generalized knowledge.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the development of large-scale neural networks. Some key milestones include:\n",
            "\n",
            "- **2012**: The success of AlexNet in the ImageNet competition demonstrated the power of deep learning and pre-trained models.\n",
            "- **2014**: The introduction of transfer learning techniques in natural language processing (NLP) with models like Word2Vec and GloVe.\n",
            "- **2018**: The release of BERT (Bidirectional Encoder Representations from Transformers) by Google, which showcased the effectiveness of fine-tuning in NLP tasks. BERT was pre-trained on a large corpus and then fine-tuned for specific tasks, setting new benchmarks in various NLP benchmarks.\n",
            "\n",
            "Fine-tuning has since become a standard practice in both computer vision and NLP, among other fields, due to its efficiency and effectiveness.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents refer to systems or applications that leverage Large Language Models (LLMs) to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, question answering, and more. LLMs, such as OpenAI's GPT-3, are trained on vast amounts of text data and can generate human-like text based on the input they receive.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Processing (NLP):** They excel in understanding and generating human language.\n",
            "2. **Versatility:** They can be used in a wide range of applications, from chatbots to content creation tools.\n",
            "3. **Contextual Understanding:** They can maintain context over longer conversations or documents.\n",
            "4. **Learning from Data:** They improve as they are exposed to more data and fine-tuning.\n",
            "\n",
            "### Timeline and Breakthroughs:\n",
            "- **Pre-2018:** Early NLP models like Word2Vec, GloVe, and initial versions of recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) were used for language tasks.\n",
            "- **2018:** The introduction of the Transformer architecture by Vaswani et al. revolutionized NLP. This architecture became the foundation for many subsequent models.\n",
            "- **2018:** OpenAI released GPT (Generative Pre-trained Transformer), which demonstrated the potential of large-scale pre-training followed by fine-tuning.\n",
            "- **2019:** OpenAI released GPT-2, which was significantly larger and more powerful than its predecessor. It gained attention for its ability to generate coherent and contextually relevant text.\n",
            "- **2020:** OpenAI released GPT-3, which further pushed the boundaries with 175 billion parameters. GPT-3's capabilities in generating human-like text and performing a wide range of tasks brought LLM-based agents into the mainstream.\n",
            "- **2021 and Beyond:** Continued advancements in LLMs, including fine-tuning techniques, prompt engineering, and the development of specialized models for specific tasks, have further solidified the role of LLM-based agents in various industries.\n",
            "\n",
            "### Applications:\n",
            "- **Customer Support:** Chatbots and virtual assistants that can handle customer queries.\n",
            "- **Content Creation:** Tools for generating articles, reports, and creative writing.\n",
            "- **Education:** Personalized tutoring and educational content generation.\n",
            "- **Healthcare:** Assisting in medical documentation and providing information.\n",
            "- **Research:** Summarizing scientific papers and generating hypotheses.\n",
            "\n",
            "The breakthrough of LLM-based agents into the mainstream can be largely attributed to the release of GPT-3 in 2020, which showcased the immense potential of these models in practical applications.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is what is string? and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: A \"string\" can refer to different concepts depending on the context:\n",
            "\n",
            "1. **In Computer Science**:\n",
            "   - A string is a sequence of characters, typically used to represent text. For example, \"Hello, World!\" is a string. Strings are fundamental data types in most programming languages and are used for storing and manipulating text.\n",
            "\n",
            "2. **In Physics (String Theory)**:\n",
            "   - String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects known as strings. These strings can vibrate at different frequencies, and their different vibrational modes correspond to different particles. String theory aims to reconcile general relativity and quantum mechanics, potentially providing a unified description of all fundamental forces and particles.\n",
            "\n",
            "### When Did String Theory Break Onto the Scene?\n",
            "\n",
            "String theory first emerged in the late 1960s and early 1970s. The initial development was aimed at describing the strong nuclear force. However, it was later realized that string theory had the potential to describe all fundamental forces, including gravity. The theory gained significant attention in the mid-1980s with the so-called \"First Superstring Revolution,\" which saw the discovery of five consistent superstring theories. The \"Second Superstring Revolution\" in the mid-1990s, led by the discovery of dualities and M-theory, further solidified its importance in theoretical physics.\n",
            "\n",
            "Would you like more detailed information on either of these topics?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
