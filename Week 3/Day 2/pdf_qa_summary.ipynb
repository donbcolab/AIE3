{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì± Building Robust RAG Systems step by step! ü§ñ\n",
    "\n",
    "- In this exciting notebook, we'll walk through creating an advanced Retrieval Augmented Generation (RAG) system to intelligently answer questions about building effective RAG solutions.\n",
    "- Get ready to level up your knowledge retrieval skills! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data...\n",
    "\n",
    "- the items in blue simply show some of my early decisions\n",
    "- due to the standardization and flexibility of the LangChain APIs I was able to experiment üî¨\n",
    "\n",
    "![image.png](./diagrams/langchain-rag-loader.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the Data...\n",
    "\n",
    "- important to remember to choose the same Embedding Model for the retriever that was used to load the data\n",
    "\n",
    "![image.png](./diagrams/langchain-rag-retriever.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf pymupdf \n",
    "%pip install -qU langchain langchain-core langchain-community langchain-experimental langchain-text-splitters \n",
    "%pip install -qU langchain-openai langchain-cohere\n",
    "%pip install -qU langchain-groq langchain-anthropic\n",
    "%pip install -qU langchain-chroma langchain-qdrant langchain-pinecone faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Assembling Our AI Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=1)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "# LangSmith tracing and \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG Architecture Amplified\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "\n",
    "# Leverage a prompt from the LangChain hub\n",
    "LLAMA3_PROMPT = hub.pull(\"rlm/rag-prompt-llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize some stuff\n",
    "\n",
    "LOAD_NEW_DATA = True\n",
    "# FILE_PATH = \"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2309.15217\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2405.17813\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2406.05085\"\n",
    "FILE_PATH = \"https://arxiv.org/pdf/2212.10496\"\n",
    "COLLECTION_NAME = \"rag_evaluation\"\n",
    "QUESTION = \"provide a step by step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Piecing Together the Perfect RAG System\n",
    "\n",
    "Building a high-performance RAG system is like solving a complex puzzle. Each piece - the document loader, text splitter, embeddings, and vector store - must be carefully chosen to fit together seamlessly.\n",
    "\n",
    "In this section, we'll walk through the key implementation choices we've made for each component, and how they contribute to a powerful, efficient, and flexible RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Intelligent Document Loading\n",
    "- **PyMuPDFLoader**: For lightning-fast processing of complex PDFs \n",
    "- **UnstructuredHTMLLoader**: When web pages are the name of the game\n",
    "- **CSVLoader**: Tabular data? No problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader Concepts - https://python.langchain.com/v0.2/docs/concepts/#document-loaders\n",
    "# PDF: https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/\n",
    "# HTML:  https://python.langchain.com/v0.2/docs/how_to/document_loader_html/\n",
    "# Microsoft Office files:  https://python.langchain.com/v0.2/docs/how_to/document_loader_office_file/\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    BSHTMLLoader,\n",
    "    SpiderLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    CSVLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the PyMuPDFLoader for its speed, ability to handle complex PDFs, and more extensive metadata.\n",
    "\n",
    "DOCUMENT_LOADER = PyMuPDFLoader\n",
    "# DOCUMENT_LOADER = \"PyPDFLoader\"\n",
    "# DOCUMENT_LOADER = \"DirectoryLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"BSHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"SpiderLoader\"\n",
    "# DOCUMENT_LOADER = \"JSONLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredMarkdownLoader\"\n",
    "# DOCUMENT_LOADER = \"CSVLoader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Strategic Text Splitting\n",
    "- **RecursiveCharacterTextSplitter**: The smart way to keep related info together\n",
    "- **TokenTextSplitter**: For when token limits matter most\n",
    "- **HuggingFaceTextSplitter**: Leveraging the best in NLP for optimal splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitters concepts - https://python.langchain.com/v0.2/docs/concepts/#text-splitters\n",
    "# Splitting by Token using HF tokenizers:  https://python.langchain.com/v0.2/docs/how_to/split_by_token/#hugging-face-tokenizer\n",
    "# Use of RecursiveCharacterTextSplitter to split code - https://python.langchain.com/v0.2/docs/how_to/code_splitter/\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    "    Language,\n",
    ")\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the text splitter to use\n",
    "# worth investigating using the RecursiveCharacterTextSplitter with the length_function based on a tokenizer VS the TokenTextSplitter\n",
    "\n",
    "TEXT_SPLITTER = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )\n",
    "# TEXT_SPLITTER = TokenTextSplitter\n",
    "# TEXT_SPLITTER = MarkdownHeaderTextSplitter\n",
    "# TEXT_SPLITTER = RecursiveJsonSplitter\n",
    "# TEXT_SPLITTER = SemanticChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™¢ Powerful Embeddings\n",
    "- **OpenAIEmbeddings**: Harnessing the power of cutting-edge language models\n",
    "- **CohereEmbeddings**: When diversity and flexibility are key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Concepts - https://python.langchain.com/v0.2/docs/concepts/#embedding-models\n",
    "# Text Embedding Models - https://python.langchain.com/v0.2/docs/how_to/embed_text/\n",
    "# Hugging Face embeddings supported through langchain-huggingface python library\n",
    "# note ability to cache embeddings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the embedding model to use\n",
    "EMBEDDING_MODEL = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    "    )\n",
    "# EMBEDDING_MODEL = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Blazing-Fast Vector Stores\n",
    "- **Qdrant**: The high-performance, scalable choice for demanding workloads\n",
    "- **Chroma**: Unbeatable speed and efficiency for real-time use cases\n",
    "- **Pinecone**: Fully-managed simplicity and reliability at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donbr/aie3-bootcamp/AIE3/Week 3/Day 2/.venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import vector stores - https://python.langchain.com/v0.2/docs/concepts/#vector-stores\n",
    "# after installing additional python dependencies, I started seeing protobuf errors with the Chroma vector store\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_pinecone import Pinecone\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Vector Store client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant client instance\n",
    "client = QdrantClient(url=QDRANT_API_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)\n",
    "\n",
    "# Initialize the Qdrant vector store\n",
    "qdrant = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embeddings=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Time for New Docs? Let's Check!\n",
    "\n",
    "The `LOAD_NEW_DATA` flag is a key part of our simple data ingestion pipeline. When set to `True`, it allows the loading of new documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Ingesting Fresh Docs: Embracing Adaptability \n",
    "\n",
    "By using a flag like `LOAD_NEW_DATA`, we can control when new data is ingested without modifying the code itself. This supports rapid experimentation and iteration, as we can test our RAG system with different datasets by simply toggling the flag.\n",
    "\n",
    "In this case, we're using `PyMuPDFLoader` to load a PDF file, but the beauty of this setup is that we can easily switch to other loaders like `UnstructuredHTMLLoader` for HTML files or `CSVLoader` for CSV data by changing the `DOCUMENT_LOADER` variable. This flexibility is crucial for adapting our pipeline to experiment with various data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run loader if LOAD_NEW_DATA is True\n",
    "if LOAD_NEW_DATA:\n",
    "    loader = DOCUMENT_LOADER(FILE_PATH)\n",
    "    docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(docs): 11\n",
      "\n",
      "docs[0].page_content[0:100]:\n",
      "Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Luyu Gao‚àó‚Ä†\n",
      "Xueguang Ma‚àó‚Ä°\n",
      "Jimmy Lin‚Ä°\n",
      "Jamie\n",
      "\n",
      "docs[0].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "docs[1].page_content[0:100]:\n",
      "HyDE\n",
      "GPT\n",
      "Contriever\n",
      "how long does it take to remove\n",
      "wisdom tooth\n",
      "It usually takes between 30\n",
      "minutes\n",
      "\n",
      "docs[1].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "docs[-2].page_content[0:100]:\n",
      "drew M. Dai, and Quoc V. Le. 2022. Finetuned lan-\n",
      "guage models are zero-shot learners. In The Tenth\n",
      "\n",
      "\n",
      "docs[-2].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "docs[-1].page_content[0:100]:\n",
      "A\n",
      "Appendix\n",
      "A.1\n",
      "Instructions\n",
      "A.1.1\n",
      "Web Search\n",
      "Please write a passage to answer the question\n",
      "Question:\n",
      "\n",
      "docs[-1].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# Document Loader validation\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(docs): {len(docs)}\")\n",
    "    print(f\"\\ndocs[0].page_content[0:100]:\\n{docs[0].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[0].metadata):\\n{docs[0].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[1].page_content[0:100]:\\n{docs[1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[1].metadata):\\n{docs[1].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-2].page_content[0:100]:\\n{docs[-2].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-2].metadata):\\n{docs[-2].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-1].page_content[0:100]:\\n{docs[-1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-1].metadata):\\n{docs[-1].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Intelligent Text Splitting\n",
    "\n",
    "Once our data is loaded, the next step is splitting it into manageable chunks. We're using the `RecursiveCharacterTextSplitter` for this, which intelligently splits text while keeping related pieces together.\n",
    "\n",
    "The splitter works by recursively dividing the text on specified characters (like newlines and periods) until each chunk is within our desired `chunk_size`. The `chunk_overlap` parameter ensures some overlap between chunks to maintain context.\n",
    "\n",
    "By adjusting these parameters, we can fine-tune the output to suit our specific use case. For example, a larger `chunk_size` results in fewer, longer chunks, while more `chunk_overlap` helps preserve context across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_NEW_DATA:\n",
    "    text_splitter = TEXT_SPLITTER\n",
    "    splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(splits): 53\n",
      "\n",
      "splits[0]:\n",
      "page_content='Precise Zero-Shot Dense Retrieval without Relevance Labels\\nLuyu Gao‚àó‚Ä†\\nXueguang Ma‚àó‚Ä°\\nJimmy Lin‚Ä°\\nJamie Callan‚Ä†\\n‚Ä†Language Technologies Institute, Carnegie Mellon University\\n‚Ä°David R. Cheriton School of Computer Science, University of Waterloo\\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\\nAbstract\\nWhile dense retrieval has been shown effec-\\ntive and efÔ¨Åcient across tasks and languages,\\nit remains difÔ¨Åcult to create effective fully\\nzero-shot dense retrieval systems when no rel-\\nevance label is available.\\nIn this paper, we\\nrecognize the difÔ¨Åculty of zero-shot learning\\nand encoding relevance.\\nInstead, we pro-\\npose to pivot through Hypothetical Document\\nEmbeddings (HyDE). Given a query, HyDE Ô¨Årst\\nzero-shot instructs an instruction-following\\nlanguage model (e.g. InstructGPT) to gen-\\nerate a hypothetical document.\\nThe docu-\\nment captures relevance patterns but is unreal\\nand may contain false details. Then, an un-\\nsupervised contrastively learned encoder (e.g.' metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "splits[1]:\n",
      "page_content='erate a hypothetical document.\\nThe docu-\\nment captures relevance patterns but is unreal\\nand may contain false details. Then, an un-\\nsupervised contrastively learned encoder (e.g.\\nContriever) encodes the document into an\\nembedding vector.\\nThis vector identiÔ¨Åes a\\nneighborhood in the corpus embedding space,\\nwhere similar real documents are retrieved\\nbased on vector similarity. This second step\\nground the generated document to the actual\\ncorpus, with the encoder‚Äôs dense bottleneck\\nÔ¨Åltering out the incorrect details. Our exper-\\niments show that HyDE signiÔ¨Åcantly outper-\\nforms the state-of-the-art unsupervised dense\\nretriever Contriever and shows strong per-\\nformance comparable to Ô¨Åne-tuned retrievers,\\nacross various tasks (e.g. web search, QA, fact\\nveriÔ¨Åcation) and languages (e.g. sw, ko, ja).1\\n1\\nIntroduction\\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\\n2020), the method of retrieving documents using\\nsemantic embedding similarities, has been shown' metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "splits[-2]:\n",
      "page_content='drew M. Dai, and Quoc V. Le. 2022. Finetuned lan-\\nguage models are zero-shot learners. In The Tenth\\nInternational Conference on Learning Representa-\\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\\nOpenReview.net.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\\nArnold Overwijk. 2021. Approximate nearest neigh-\\nbor negative contrastive learning for dense text re-\\ntrieval. In 9th International Conference on Learning\\nRepresentations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net.\\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\\nArnold Overwijk. 2022. Coco-dr: Combating dis-\\ntribution shifts in zero-shot dense retrieval with con-\\ntrastive and distributionally robust learning. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing.\\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.\\n2021.\\nMr. TyDi: A multi-lingual benchmark for\\ndense retrieval. arXiv:2108.08787.' metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "splits[-1]:\n",
      "page_content='A\\nAppendix\\nA.1\\nInstructions\\nA.1.1\\nWeb Search\\nPlease write a passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.2\\nSciFact\\nPlease write a scientiÔ¨Åc paper passage to support/refute the claim\\nClaim: [Claim]\\nPassage:\\nA.1.3\\nArguana\\nPlease write a counter argument for the passage\\nPassage: [PASSAGE]\\nCounter Argument:\\nA.1.4\\nTREC-COVID\\nPlease write a scientiÔ¨Åc paper passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.5\\nFiQA\\nPlease write a Ô¨Ånancial article passage to answer the question\\nQuestion: [QUESTION]\\nPassage:\\nA.1.6\\nDBPedia-Entity\\nPlease write a passage to answer the question.\\nQuestion: [QUESTION]\\nPassage:\\nA.1.7\\nTREC-NEWS\\nPlease write a news passage about the topic.\\nTopic: [TOPIC]\\nPassage:\\nA.1.8\\nMr.TyDi\\nPlease write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\\nQuestion: [QUESTION]\\nPassage:' metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'file_path': 'https://arxiv.org/pdf/2212.10496', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221014304Z', 'modDate': 'D:20221221014304Z', 'trapped': ''}\n",
      "\n",
      "Split # 0:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[0]): 976\n",
      "splits[0][0:25]: Precise Zero-Shot Dense R\n",
      "\n",
      "Split # 1:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[1]): 964\n",
      "splits[1][0:25]: erate a hypothetical docu\n",
      "\n",
      "Split # 2:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[2]): 991\n",
      "splits[2][0:25]: 1\n",
      "Introduction\n",
      "Dense retr\n",
      "\n",
      "Split # 3:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[3]): 992\n",
      "splits[3][0:25]: remains difÔ¨Åcult. Many re\n",
      "\n",
      "Split # 4:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[4]): 989\n",
      "splits[4][0:25]: tion learning methods. Mo\n",
      "\n",
      "Split # 5:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[5]): 971\n",
      "splits[5][0:25]: HyDE\n",
      "GPT\n",
      "Contriever\n",
      "how l\n",
      "\n",
      "Split # 6:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[6]): 964\n",
      "splits[6][0:25]: real document\n",
      "Figure 1: A\n",
      "\n",
      "Split # 7:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[7]): 984\n",
      "splits[7][0:25]: document is not real, can\n",
      "\n",
      "Split # 8:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[8]): 949\n",
      "splits[8][0:25]: in HyDE: both the generat\n",
      "\n",
      "Split # 9:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[9]): 985\n",
      "splits[9][0:25]: loss (Karpukhin et al., 2\n",
      "\n",
      "Split # 10:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[10]): 517\n",
      "splits[10][0:25]: instructions (Ouyang et a\n",
      "\n",
      "Split # 11:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[11]): 993\n",
      "splits[11][0:25]: instruction following gen\n",
      "\n",
      "Split # 12:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[12]): 981\n",
      "splits[12][0:25]: do not assume access to t\n",
      "\n",
      "Split # 13:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[13]): 993\n",
      "splits[13][0:25]: to use novel forms of sea\n",
      "\n",
      "Split # 14:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[14]): 974\n",
      "splits[14][0:25]: For zero-shot retrieval, \n",
      "\n",
      "Split # 15:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[15]): 770\n",
      "splits[15][0:25]: document similarity. This\n",
      "\n",
      "Split # 16:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[16]): 998\n",
      "splits[16][0:25]: to be ‚Äúwrite a paragraph \n",
      "\n",
      "Split # 17:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[17]): 954\n",
      "splits[17][0:25]: the expectation value, as\n",
      "\n",
      "Split # 18:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[18]): 993\n",
      "splits[18][0:25]: and the real documents. T\n",
      "\n",
      "Split # 19:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[19]): 994\n",
      "splits[19][0:25]: datasets from the BEIR da\n",
      "\n",
      "Split # 20:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[20]): 899\n",
      "splits[20][0:25]: Several systems that invo\n",
      "\n",
      "Split # 21:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[21]): 999\n",
      "splits[21][0:25]: DL19\n",
      "DL20\n",
      "map\n",
      "ndcg@10\n",
      "rec\n",
      "\n",
      "Split # 22:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[22]): 970\n",
      "splits[22][0:25]: 41.5\n",
      "65.4\n",
      "30.0\n",
      "28.1\n",
      "38.2\n",
      "\n",
      "\n",
      "Split # 23:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[23]): 991\n",
      "splits[23][0:25]: vised. On TREC DL19, HyDE\n",
      "\n",
      "Split # 24:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[24]): 370\n",
      "splits[24][0:25]: HyDE generally shows bett\n",
      "\n",
      "Split # 25:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[25]): 996\n",
      "splits[25][0:25]: Swahili\n",
      "Korean\n",
      "Japanese\n",
      "B\n",
      "\n",
      "Split # 26:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[26]): 979\n",
      "splits[26][0:25]: under-trained (Hoffmann e\n",
      "\n",
      "Split # 27:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[27]): 995\n",
      "splits[27][0:25]: Effect of Different Gener\n",
      "\n",
      "Split # 28:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[28]): 956\n",
      "splits[28][0:25]: Contriever, with larger m\n",
      "\n",
      "Split # 29:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[29]): 687\n",
      "splits[29][0:25]: Ô¨Åne-tuned for TREC DL19/2\n",
      "\n",
      "Split # 30:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[30]): 980\n",
      "splits[30][0:25]: ment. Dense retrievers co\n",
      "\n",
      "Split # 31:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[31]): 979\n",
      "splits[31][0:25]: evance modeling and instr\n",
      "\n",
      "Split # 32:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[32]): 984\n",
      "splits[32][0:25]: to HyDE backend.\n",
      "Referenc\n",
      "\n",
      "Split # 33:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[33]): 982\n",
      "splits[33][0:25]: Neelakantan, Pranav Shyam\n",
      "\n",
      "Split # 34:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[34]): 959\n",
      "splits[34][0:25]: Krueger, Michael Petrov, \n",
      "\n",
      "Split # 35:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[35]): 987\n",
      "splits[35][0:25]: Aakanksha Chowdhery, Shar\n",
      "\n",
      "Split # 36:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[36]): 430\n",
      "splits[36][0:25]: Erica Moreira, Rewon Chil\n",
      "\n",
      "Split # 37:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[37]): 987\n",
      "splits[37][0:25]: moyer, and Veselin Stoyan\n",
      "\n",
      "Split # 38:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[38]): 981\n",
      "splits[38][0:25]: for Computational Linguis\n",
      "\n",
      "Split # 39:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[39]): 965\n",
      "splits[39][0:25]: Tianyu Gao, Xingcheng Yao\n",
      "\n",
      "Split # 40:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[40]): 963\n",
      "splits[40][0:25]: Yang, Jimmy Lin, and Alla\n",
      "\n",
      "Split # 41:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[41]): 987\n",
      "splits[41][0:25]: Wen-tau Yih. 2020.\n",
      "Dense \n",
      "\n",
      "Split # 42:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[42]): 963\n",
      "splits[42][0:25]: information retrieval res\n",
      "\n",
      "Split # 43:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[43]): 470\n",
      "splits[43][0:25]: and Arnold Overwijk. 2021\n",
      "\n",
      "Split # 44:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[44]): 997\n",
      "splits[44][0:25]: Sewon Min, Mike Lewis, Lu\n",
      "\n",
      "Split # 45:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[45]): 961\n",
      "splits[45][0:25]: and Haifeng Wang. 2021.\n",
      "R\n",
      "\n",
      "Split # 46:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[46]): 964\n",
      "splits[46][0:25]: tonia Creswell, Nat McAle\n",
      "\n",
      "Split # 47:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[47]): 975\n",
      "splits[47][0:25]: way, Lorrayne Bennett, De\n",
      "\n",
      "Split # 48:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[48]): 970\n",
      "splits[48][0:25]: heesht Sharma, Andrea San\n",
      "\n",
      "Split # 49:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[49]): 974\n",
      "splits[49][0:25]: CoRR,\n",
      "abs/2104.08663.\n",
      "Rom\n",
      "\n",
      "Split # 50:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[50]): 679\n",
      "splits[50][0:25]: Aaron Cohen, Rachel Berns\n",
      "\n",
      "Split # 51:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[51]): 975\n",
      "splits[51][0:25]: drew M. Dai, and Quoc V. \n",
      "\n",
      "Split # 52:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[52]): 856\n",
      "splits[52][0:25]: A\n",
      "Appendix\n",
      "A.1\n",
      "Instructio\n"
     ]
    }
   ],
   "source": [
    "# capture the split chunks for use in the vector store\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(splits): {len(splits)}\")\n",
    "\n",
    "    print(f\"\\nsplits[0]:\\n{splits[0]}\")\n",
    "    print(f\"\\nsplits[1]:\\n{splits[1]}\")\n",
    "    print(f\"\\nsplits[-2]:\\n{splits[-2]}\")\n",
    "    print(f\"\\nsplits[-1]:\\n{splits[-1]}\")\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        print(f\"\\nSplit # {i}:\")\n",
    "        # print page number from split.metadata\n",
    "\n",
    "        print(f\"split.metadata.get('page'): {split.metadata.get('page')}\")\n",
    "        print(f\"len(splits[{i}]): {len(split.page_content)}\")\n",
    "        print(f\"splits[{i}][0:25]: {split.page_content[0:25]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Supercharging Our RAG System with Qdrant\n",
    "\n",
    "With our text split into manageable chunks, it's time to vectorize and store them for fast retrieval. That's where Qdrant comes in - a state-of-the-art vector database that offers unparalleled performance, scalability, and flexibility.\n",
    "\n",
    "Qdrant utilizes the HNSW algorithm for blazing-fast similarity search, delivering up to 4x higher requests per second compared to alternatives. Its advanced compression features reduce memory usage by up to 97%, while its flexible storage options allow us to fine-tune for our specific needs.\n",
    "\n",
    "But Qdrant isn't just fast - it's also incredibly versatile. With support for hybrid search (combining vector similarity and filtering), sparse vectors, and rich JSON payloads, Qdrant enables powerful querying patterns that go beyond simple similarity search.\n",
    "\n",
    "And with a robust set of enterprise features like multitenancy, access control, and backup/recovery, Qdrant is ready to scale with our RAG system as it grows.\n",
    "\n",
    "By leveraging Qdrant's speed, efficiency, and flexibility, we're building a knowledge base that can rapidly retrieve the most relevant information for any query. Whether we're serving a small prototype or a massive production system, Qdrant has us covered.\n",
    "\n",
    "So let's dive in and see how Qdrant can supercharge our RAG system! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the chunks in Qdrant\n",
    "if LOAD_NEW_DATA:\n",
    "    from_splits = qdrant.from_documents(\n",
    "        url=QDRANT_API_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "        prefer_grpc=True,\n",
    "        documents=splits,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=EMBEDDING_MODEL\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Implementing a Robust Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concepts:  https://python.langchain.com/v0.2/docs/concepts/#retrievers\n",
    "# Vector Store as Retriever:  https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/\n",
    "# Including Similarity Search Scores:  https://python.langchain.com/v0.2/docs/how_to/add_scores_retriever/\n",
    "\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Constructing the RAG Chain for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": LLAMA3_PROMPT | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      +---------------------------------+                        \n",
      "                      | Parallel<context,question>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                           ****                   ****                           \n",
      "                       ****                           ***                        \n",
      "                     **                                  ****                    \n",
      "+--------------------------------+                           **                  \n",
      "| Lambda(itemgetter('question')) |                            *                  \n",
      "+--------------------------------+                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "     | VectorStoreRetriever |                 | Lambda(itemgetter('question')) | \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "                           ****                   ****                           \n",
      "                               ****           ****                               \n",
      "                                   **       **                                   \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<context,question>Output |                       \n",
      "                      +----------------------------------+                       \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                           +------------------------+                            \n",
      "                           | Parallel<context>Input |                            \n",
      "                           +------------------------+                            \n",
      "                              ***               ***                              \n",
      "                           ***                     ***                           \n",
      "                         **                           **                         \n",
      "     +-------------------------------+            +-------------+                \n",
      "     | Lambda(itemgetter('context')) |            | Passthrough |                \n",
      "     +-------------------------------+            +-------------+                \n",
      "                              ***               ***                              \n",
      "                                 ***         ***                                 \n",
      "                                    **     **                                    \n",
      "                          +-------------------------+                            \n",
      "                          | Parallel<context>Output |                            \n",
      "                          +-------------------------+                            \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                      +---------------------------------+                        \n",
      "                      | Parallel<response,context>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                            ****                 ***                             \n",
      "                         ***                        ***                          \n",
      "                       **                              ****                      \n",
      "         +--------------------+                            **                    \n",
      "         | ChatPromptTemplate |                             *                    \n",
      "         +--------------------+                             *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "              +----------+                 +-------------------------------+     \n",
      "              | ChatGroq |                 | Lambda(itemgetter('context')) |     \n",
      "              +----------+**               +-------------------------------+     \n",
      "                            ****                ****                             \n",
      "                                ***          ***                                 \n",
      "                                   **      **                                    \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<response,context>Output |                       \n",
      "                      +----------------------------------+                       \n"
     ]
    }
   ],
   "source": [
    "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Moment of Truth: Testing Our RAG System!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : QUESTION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a step-by-step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions:\\n\\n1. Identify the evaluation metrics: Define the key performance indicators (KPIs) that align with the business objectives, such as accuracy, relevance, and fluency.\\n2. Select a reference-free evaluation framework: Utilize a framework like RAGAS, which provides a reference-free evaluation approach for RAG pipelines, considering multiple dimensions, including the retrieval system's ability to identify relevant context passages and the LLM's ability to exploit these passages.\\n3. Automate the evaluation process: Develop an automated evaluation process that can assess the RAG system's performance on various metrics, ensuring consistency and reproducibility.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the response.  filter on the response key AIMessage content element\n",
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a step-by-step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions:\n",
      "\n",
      "1. Identify the evaluation metrics: Define the key performance indicators (KPIs) that align with the business objectives, such as accuracy, relevance, and fluency.\n",
      "2. Select a reference-free evaluation framework: Utilize a framework like RAGAS, which provides a reference-free evaluation approach for RAG pipelines, considering multiple dimensions, including the retrieval system's ability to identify relevant context passages and the LLM's ability to exploit these passages.\n",
      "3. Automate the evaluation process: Develop an automated evaluation process that can assess the RAG system's performance on various metrics, ensuring consistency and reproducibility.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vector store CONTEXT # 0:\n",
      "Page # : 0\n",
      "context.page_content:\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es‚Ä†, Jithin James‚Ä†, Luis Espinosa-Anke‚àó‚ô¢, Steven Schockaert‚àó\n",
      "‚Ä†Exploding Gradients\n",
      "‚àóCardiffNLP, Cardiff University, United Kingdom\n",
      "‚ô¢AMPLYFI, United Kingdom\n",
      "shahules786@gmail.com,jamesjithin97@gmail.com\n",
      "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "Abstract\n",
      "We introduce RAGAS (Retrieval Augmented\n",
      "Generation Assessment), a framework for\n",
      "reference-free evaluation of Retrieval Aug-\n",
      "mented Generation (RAG) pipelines.\n",
      "RAG\n",
      "systems are composed of a retrieval and an\n",
      "LLM based generation module, and provide\n",
      "LLMs with knowledge from a reference textual\n",
      "database, which enables them to act as a natu-\n",
      "ral language layer between a user and textual\n",
      "databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal-\n",
      "lenging because there are several dimensions to\n",
      "consider: the ability of the retrieval system to\n",
      "identify relevant and focused context passages,\n",
      "the ability of the LLM to exploit such passages\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 8, 'keywords': '', 'modDate': 'D:20230928011700Z', 'trapped': '', 'format': 'PDF 1.5', 'creationDate': 'D:20230928011700Z', 'source': 'https://arxiv.org/pdf/2309.15217', 'file_path': 'https://arxiv.org/pdf/2309.15217', 'producer': 'pdfTeX-1.40.25', 'page': 0, 'title': '', 'author': '', '_id': '198744fc-4ae2-4a29-a2d8-5ddd67fc584a', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 1:\n",
      "Page # : 0\n",
      "context.page_content:\n",
      "a significant amount of tuning, as the overall per-\n",
      "formance will be affected by the retrieval model,\n",
      "the considered corpus, the LM, or the prompt for-\n",
      "mulation, among others. Automated evaluation of\n",
      "retrieval-augmented systems is thus paramount. In\n",
      "practice, RAG systems are often evaluated in terms\n",
      "of the language modelling task itself, i.e. by mea-\n",
      "suring perplexity on some reference corpus. How-\n",
      "ever, such evaluations are not always predictive\n",
      "of downstream performance (Wang et al., 2023c).\n",
      "Moreover, this evaluation strategy relies on the LM\n",
      "probabilities, which are not accessible for some\n",
      "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
      "tion answering is another common evaluation task,\n",
      "but usually only datasets with short extractive an-\n",
      "swers are considered, which may not be represen-\n",
      "tative of how the system will be used.\n",
      "To address these issues, in this paper we present\n",
      "RAGAS1, a framework for the automated assess-\n",
      "1RAGAS\n",
      "is\n",
      "available\n",
      "at\n",
      "https://github.com/\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 8, 'keywords': '', 'trapped': '', 'modDate': 'D:20230928011700Z', 'format': 'PDF 1.5', 'file_path': 'https://arxiv.org/pdf/2309.15217', 'creationDate': 'D:20230928011700Z', 'source': 'https://arxiv.org/pdf/2309.15217', 'title': '', 'page': 0, 'producer': 'pdfTeX-1.40.25', 'author': '', '_id': 'ec239e82-35d6-49c7-9ddd-30cfe3eaffd8', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 2:\n",
      "Page # : 1\n",
      "context.page_content:\n",
      "hallucinations (by grounding the LLM reply in reliable sources), and ensuring that responses contain\n",
      "up-to-date knowledge (e.g., by accessing the Internet), all without requiring expensive training.\n",
      "More specifically, there are two main stages in a RAG pipeline: data preparation and query execution.\n",
      "During data preparation, one constructs a vector database (DB) populated with embeddings and their\n",
      "corresponding data items such as documents. During query execution, one constructs an embedding\n",
      "of that query and retrieves data items in the store with similar embeddings.\n",
      "Intense recent research efforts have been put into RAG [10, 12, 14, 20, 25, 41, 45]. On one hand,\n",
      "different RAG designs have been proposed, for example RAPTOR [31], Self-RAG [2], Chain-of-\n",
      "Note [42], and many others [1, 6, 7, 23, 35, 39, 43]. In general, these schemes focus on making the\n",
      "retrieved data more accurate and relevant to the query. On the other hand, there have also been efforts\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 14, 'keywords': '', 'trapped': '', 'modDate': 'D:20240610005547Z', 'format': 'PDF 1.5', 'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'creationDate': 'D:20240610005547Z', 'producer': 'pdfTeX-1.40.25', 'page': 1, 'title': '', 'author': '', '_id': '60be6ace-069f-4fd0-908f-b63731842460', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 3:\n",
      "Page # : 8\n",
      "context.page_content:\n",
      "relationships or the inner organization of text. Usually, they need a sophisticated preprocessing phase\n",
      "to prepare such structures. MRAG achieves the improvement solely based on the embedding model\n",
      "and has no additional storage requirements, and can be combined with any of these schemes.\n",
      "6\n",
      "Conclusion\n",
      "Retrieval Augmented Generation (RAG) is pivotal for democratizing access to accurate and relevant\n",
      "outputs from large language models (LLMs). Enhancing the precision and relevance of these outputs\n",
      "is a critical goal, especially given the challenges posed by queries requiring the retrieval of multiple\n",
      "9\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 14, 'keywords': '', 'trapped': '', 'modDate': 'D:20240610005547Z', 'format': 'PDF 1.5', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'source': 'https://arxiv.org/pdf/2406.05085', 'creationDate': 'D:20240610005547Z', 'producer': 'pdfTeX-1.40.25', 'page': 8, 'title': '', 'author': '', '_id': 'd7f9d8c3-b3a1-4fe3-a3b9-d41d5686985e', '_collection_name': 'rag_evaluation'}\n"
     ]
    }
   ],
   "source": [
    "for i, context_instance in enumerate(response[\"context\"]):\n",
    "  print(f\"\\nvector store CONTEXT # {i}:\")\n",
    "  print(f\"Page # : {context_instance.metadata.get('page')}\")\n",
    "  print(f\"context.page_content:\\n{context_instance.page_content}\")\n",
    "  print(f\"context.metadata:\\n{context_instance.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
