{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì± Building Robust RAG Systems step by step! ü§ñ\n",
    "\n",
    "- In this exciting notebook, we'll walk through creating an advanced Retrieval Augmented Generation (RAG) system to intelligently answer questions about building effective RAG solutions.\n",
    "- Get ready to level up your knowledge retrieval skills! üöÄ\n",
    "- Checkout the notes under the \"Piecing Together the Perfect RAG System\" section for information on LangChain v0.2 docs.  Watch out for those breaking changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you'll learn üöÄ\n",
    "\n",
    "- üèóÔ∏è LangSmith - Set up a crude end-to-end framework to test and evaluate the RAG solution\n",
    "- üîç Qdrant - initialize a vector store retriever that can run independently from the document loader üí™\n",
    "- üé® LCEL - the ascii art helps bring this concept home, and confirm that the flow is set-up properly üìö\n",
    "\n",
    "### For next time...\n",
    "\n",
    "- üß† Some of the key themes I'll do a deep dive on soon are captured [in this LangSmith Trace](https://smith.langchain.com/public/aebb43d1-5b39-42ad-ac22-f20c3efcda1b/r) üí°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data...\n",
    "\n",
    "- the items in blue simply show some of my early decisions\n",
    "- due to the standardization and flexibility of the LangChain APIs I was able to experiment üî¨\n",
    "\n",
    "![image.png](./diagrams/langchain-rag-loader.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the Data...\n",
    "\n",
    "- important to remember to choose the same Embedding Model for the retriever that was used to load the data\n",
    "\n",
    "![image.png](./diagrams/langchain-rag-retriever.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I'm excited to learn next! üéâ\n",
    "\n",
    "\n",
    "- go back and revisit some of the alternatives at each step of the RAG data pipeline (document loader, test splitter, embedding models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Assembling Our AI Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf pymupdf \n",
    "%pip install -qU langchain langchain-core langchain-community langchain-experimental langchain-text-splitters \n",
    "%pip install -qU langchain-openai langchain-cohere\n",
    "%pip install -qU langchain-groq langchain-anthropic\n",
    "%pip install -qU langchain-chroma langchain-qdrant langchain-pinecone faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0.1)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "# LangSmith tracing and \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AirBnB PDF Jun18\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "\n",
    "# Leverage a prompt from the LangChain hub\n",
    "LLAMA3_PROMPT = hub.pull(\"rlm/rag-prompt-llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize some stuff\n",
    "\n",
    "LOAD_NEW_DATA = False\n",
    "# FILE_PATH = \"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2309.15217\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2405.17813\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2406.05085\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2212.10496\"\n",
    "FILE_PATH = \"/home/donbr/aie3-bootcamp/AIE3/Week 3/Day 2/files/airbnb.pdf\"\n",
    "COLLECTION_NAME = \"airbnb_pdf_rec_1000_200\"\n",
    "# QUESTION = \"provide a step by step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions.\"\n",
    "# QUESTION = \"What is Airbnb's 'Description of Business'?\"\n",
    "# QUESTION = \"What was the total value of 'Cash and cash equivalents' as of December 31, 2023?\"\n",
    "QUESTION = \"What is the 'maximum number of shares to be sold under the 10b5-1 Trading plan' by Brian Chesky?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Piecing Together the Perfect RAG System\n",
    "\n",
    "Building a high-performance RAG system is like solving a complex puzzle. Each piece - the document loader, text splitter, embeddings, and vector store - must be carefully chosen to fit together seamlessly.\n",
    "\n",
    "In this section, we'll walk through the key implementation choices we've made for each component, and how they contribute to a powerful, efficient, and flexible RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Intelligent Document Loading\n",
    "- **PyMuPDFLoader**: For lightning-fast processing of complex PDFs \n",
    "- **UnstructuredHTMLLoader**: When web pages are the name of the game\n",
    "- **CSVLoader**: Tabular data? No problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader Concepts - https://python.langchain.com/v0.2/docs/concepts/#document-loaders\n",
    "# PDF: https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/\n",
    "# HTML:  https://python.langchain.com/v0.2/docs/how_to/document_loader_html/\n",
    "# Microsoft Office files:  https://python.langchain.com/v0.2/docs/how_to/document_loader_office_file/\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    BSHTMLLoader,\n",
    "    SpiderLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    CSVLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the PyMuPDFLoader for its speed, ability to handle complex PDFs, and more extensive metadata.\n",
    "\n",
    "DOCUMENT_LOADER = PyMuPDFLoader\n",
    "# DOCUMENT_LOADER = \"PyPDFLoader\"\n",
    "# DOCUMENT_LOADER = \"DirectoryLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"BSHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"SpiderLoader\"\n",
    "# DOCUMENT_LOADER = \"JSONLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredMarkdownLoader\"\n",
    "# DOCUMENT_LOADER = \"CSVLoader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Strategic Text Splitting\n",
    "- **RecursiveCharacterTextSplitter**: The smart way to keep related info together\n",
    "- **TokenTextSplitter**: For when token limits matter most\n",
    "- **HuggingFaceTextSplitter**: Leveraging the best in NLP for optimal splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitters concepts - https://python.langchain.com/v0.2/docs/concepts/#text-splitters\n",
    "# Splitting by Token using HF tokenizers:  https://python.langchain.com/v0.2/docs/how_to/split_by_token/#hugging-face-tokenizer\n",
    "# Use of RecursiveCharacterTextSplitter to split code - https://python.langchain.com/v0.2/docs/how_to/code_splitter/\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    "    Language,\n",
    ")\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the text splitter to use\n",
    "# worth investigating using the RecursiveCharacterTextSplitter with the length_function based on a tokenizer VS the TokenTextSplitter\n",
    "\n",
    "TEXT_SPLITTER = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )\n",
    "# TEXT_SPLITTER = TokenTextSplitter\n",
    "# TEXT_SPLITTER = MarkdownHeaderTextSplitter\n",
    "# TEXT_SPLITTER = RecursiveJsonSplitter\n",
    "# TEXT_SPLITTER = SemanticChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™¢ Powerful Embeddings\n",
    "- **OpenAIEmbeddings**: Harnessing the power of cutting-edge language models\n",
    "- **CohereEmbeddings**: When diversity and flexibility are key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Concepts - https://python.langchain.com/v0.2/docs/concepts/#embedding-models\n",
    "# Text Embedding Models - https://python.langchain.com/v0.2/docs/how_to/embed_text/\n",
    "# Hugging Face embeddings supported through langchain-huggingface python library\n",
    "# note ability to cache embeddings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the embedding model to use\n",
    "EMBEDDING_MODEL = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    "    )\n",
    "# EMBEDDING_MODEL = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Blazing-Fast Vector Stores\n",
    "- **Qdrant**: The high-performance, scalable choice for demanding workloads\n",
    "- **Chroma**: Unbeatable speed and efficiency for real-time use cases\n",
    "- **Pinecone**: Fully-managed simplicity and reliability at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vector stores - https://python.langchain.com/v0.2/docs/concepts/#vector-stores\n",
    "# after installing additional python dependencies, I started seeing protobuf errors with the Chroma vector store\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_pinecone import Pinecone\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vector Store client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant client instance\n",
    "client = QdrantClient(url=QDRANT_API_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)\n",
    "\n",
    "# Initialize the Qdrant vector store\n",
    "qdrant = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embeddings=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Time for New Docs? Let's Check!\n",
    "\n",
    "The `LOAD_NEW_DATA` flag is a key part of our simple data ingestion pipeline. When set to `True`, it allows the loading of new documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Ingesting Fresh Docs: Embracing Adaptability \n",
    "\n",
    "By using a flag like `LOAD_NEW_DATA`, we can control when new data is ingested without modifying the code itself. This supports rapid experimentation and iteration, as we can test our RAG system with different datasets by simply toggling the flag.\n",
    "\n",
    "In this case, we're using `PyMuPDFLoader` to load a PDF file, but the beauty of this setup is that we can easily switch to other loaders like `UnstructuredHTMLLoader` for HTML files or `CSVLoader` for CSV data by changing the `DOCUMENT_LOADER` variable. This flexibility is crucial for adapting our pipeline to experiment with various data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run loader if LOAD_NEW_DATA is True\n",
    "if LOAD_NEW_DATA:\n",
    "    loader = DOCUMENT_LOADER(FILE_PATH)\n",
    "    docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader validation\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(docs): {len(docs)}\")\n",
    "    print(f\"\\ndocs[0].page_content[0:100]:\\n{docs[0].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[0].metadata):\\n{docs[0].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[1].page_content[0:100]:\\n{docs[1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[1].metadata):\\n{docs[1].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-2].page_content[0:100]:\\n{docs[-2].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-2].metadata):\\n{docs[-2].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-1].page_content[0:100]:\\n{docs[-1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-1].metadata):\\n{docs[-1].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Intelligent Text Splitting\n",
    "\n",
    "Once our data is loaded, the next step is splitting it into manageable chunks. We're using the `RecursiveCharacterTextSplitter` for this, which intelligently splits text while keeping related pieces together.\n",
    "\n",
    "The splitter works by recursively dividing the text on specified characters (like newlines and periods) until each chunk is within our desired `chunk_size`. The `chunk_overlap` parameter ensures some overlap between chunks to maintain context.\n",
    "\n",
    "By adjusting these parameters, we can fine-tune the output to suit our specific use case. For example, a larger `chunk_size` results in fewer, longer chunks, while more `chunk_overlap` helps preserve context across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_NEW_DATA:\n",
    "    text_splitter = TEXT_SPLITTER\n",
    "    splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the split chunks for use in the vector store\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(splits): {len(splits)}\")\n",
    "\n",
    "    print(f\"\\nsplits[0]:\\n{splits[0]}\")\n",
    "    print(f\"\\nsplits[1]:\\n{splits[1]}\")\n",
    "    print(f\"\\nsplits[-2]:\\n{splits[-2]}\")\n",
    "    print(f\"\\nsplits[-1]:\\n{splits[-1]}\")\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        print(f\"\\nSplit # {i}:\")\n",
    "        # print page number from split.metadata\n",
    "\n",
    "        print(f\"split.metadata.get('page'): {split.metadata.get('page')}\")\n",
    "        print(f\"len(splits[{i}]): {len(split.page_content)}\")\n",
    "        print(f\"splits[{i}][0:25]: {split.page_content[0:25]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Supercharging Our RAG System with Qdrant\n",
    "\n",
    "With our text split into manageable chunks, it's time to vectorize and store them for fast retrieval. That's where Qdrant comes in - a state-of-the-art vector database that offers unparalleled performance, scalability, and flexibility.\n",
    "\n",
    "Qdrant utilizes the HNSW algorithm for blazing-fast similarity search, delivering up to 4x higher requests per second compared to alternatives. Its advanced compression features reduce memory usage by up to 97%, while its flexible storage options allow us to fine-tune for our specific needs.\n",
    "\n",
    "But Qdrant isn't just fast - it's also incredibly versatile. With support for hybrid search (combining vector similarity and filtering), sparse vectors, and rich JSON payloads, Qdrant enables powerful querying patterns that go beyond simple similarity search.\n",
    "\n",
    "And with a robust set of enterprise features like multitenancy, access control, and backup/recovery, Qdrant is ready to scale with our RAG system as it grows.\n",
    "\n",
    "By leveraging Qdrant's speed, efficiency, and flexibility, we're building a knowledge base that can rapidly retrieve the most relevant information for any query. Whether we're serving a small prototype or a massive production system, Qdrant has us covered.\n",
    "\n",
    "So let's dive in and see how Qdrant can supercharge our RAG system! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the chunks in Qdrant\n",
    "if LOAD_NEW_DATA:\n",
    "    from_splits = qdrant.from_documents(\n",
    "        url=QDRANT_API_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "        prefer_grpc=True,\n",
    "        documents=splits,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=EMBEDDING_MODEL\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Implementing a Robust Vector Store Retriever\n",
    "\n",
    "- depends on the \"Initialize the Vector Store client\" section above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concepts:  https://python.langchain.com/v0.2/docs/concepts/#retrievers\n",
    "# Vector Store as Retriever:  https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/\n",
    "# Including Similarity Search Scores:  https://python.langchain.com/v0.2/docs/how_to/add_scores_retriever/\n",
    "\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Constructing the RAG Chain for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": LLAMA3_PROMPT | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      +---------------------------------+                        \n",
      "                      | Parallel<context,question>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                           ****                   ****                           \n",
      "                       ****                           ***                        \n",
      "                     **                                  ****                    \n",
      "+--------------------------------+                           **                  \n",
      "| Lambda(itemgetter('question')) |                            *                  \n",
      "+--------------------------------+                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "     | VectorStoreRetriever |                 | Lambda(itemgetter('question')) | \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "                           ****                   ****                           \n",
      "                               ****           ****                               \n",
      "                                   **       **                                   \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<context,question>Output |                       \n",
      "                      +----------------------------------+                       \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                           +------------------------+                            \n",
      "                           | Parallel<context>Input |                            \n",
      "                           +------------------------+                            \n",
      "                              ***               ***                              \n",
      "                           ***                     ***                           \n",
      "                         **                           **                         \n",
      "     +-------------------------------+            +-------------+                \n",
      "     | Lambda(itemgetter('context')) |            | Passthrough |                \n",
      "     +-------------------------------+            +-------------+                \n",
      "                              ***               ***                              \n",
      "                                 ***         ***                                 \n",
      "                                    **     **                                    \n",
      "                          +-------------------------+                            \n",
      "                          | Parallel<context>Output |                            \n",
      "                          +-------------------------+                            \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                      +---------------------------------+                        \n",
      "                      | Parallel<response,context>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                            ****                 ***                             \n",
      "                         ***                        ***                          \n",
      "                       **                              ****                      \n",
      "         +--------------------+                            **                    \n",
      "         | ChatPromptTemplate |                             *                    \n",
      "         +--------------------+                             *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "              +----------+                 +-------------------------------+     \n",
      "              | ChatGroq |                 | Lambda(itemgetter('context')) |     \n",
      "              +----------+**               +-------------------------------+     \n",
      "                            ****                ****                             \n",
      "                                ***          ***                                 \n",
      "                                   **      **                                    \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<response,context>Output |                       \n",
      "                      +----------------------------------+                       \n"
     ]
    }
   ],
   "source": [
    "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Moment of Truth: Testing Our RAG System!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : QUESTION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The maximum number of shares to be sold under the 10b5-1 Trading plan by Brian Chesky is 1,146,000.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the response.  filter on the response key AIMessage content element\n",
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of shares to be sold under the 10b5-1 Trading plan by Brian Chesky is 1,146,000.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks to LangSmith, this custom code is no longer required\n",
    "\n",
    "```python\n",
    "for i, context_instance in enumerate(response[\"context\"]):\n",
    "  print(f\"\\nvector store CONTEXT # {i}:\")\n",
    "  print(f\"Page # : {context_instance.metadata.get('page')}\")\n",
    "  print(f\"context.page_content:\\n{context_instance.page_content}\")\n",
    "  print(f\"context.metadata:\\n{context_instance.metadata}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
