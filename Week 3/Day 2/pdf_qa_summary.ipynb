{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì± Building Robust RAG Systems step by step! ü§ñ\n",
    "\n",
    "- In this exciting notebook, we'll walk through creating an advanced Retrieval Augmented Generation (RAG) system to intelligently answer questions about building effective RAG solutions.\n",
    "- Get ready to level up your knowledge retrieval skills! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf pymupdf \n",
    "%pip install -qU langchain langchain-core langchain-community langchain-experimental langchain-text-splitters \n",
    "%pip install -qU langchain-openai langchain-cohere\n",
    "%pip install -qU langchain-groq langchain-anthropic\n",
    "%pip install -qU langchain-chroma langchain-qdrant langchain-pinecone faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Assembling Our AI Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=1)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "QDRANT_API_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "# LangSmith tracing and \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG Architecture Amplified\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "\n",
    "# Leverage a prompt from the LangChain hub\n",
    "LLAMA3_PROMPT = hub.pull(\"rlm/rag-prompt-llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize some stuff\n",
    "\n",
    "LOAD_NEW_DATA = True\n",
    "# FILE_PATH = \"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2309.15217\"\n",
    "# FILE_PATH = \"https://arxiv.org/pdf/2405.17813\"\n",
    "FILE_PATH = \"https://arxiv.org/pdf/2406.05085\"\n",
    "COLLECTION_NAME = \"rag_evaluation\"\n",
    "QUESTION = \"provide a step by step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Piecing Together the Perfect RAG System\n",
    "\n",
    "Building a high-performance RAG system is like solving a complex puzzle. Each piece - the document loader, text splitter, embeddings, and vector store - must be carefully chosen to fit together seamlessly.\n",
    "\n",
    "In this section, we'll walk through the key implementation choices we've made for each component, and how they contribute to a powerful, efficient, and flexible RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Intelligent Document Loading\n",
    "- **PyMuPDFLoader**: For lightning-fast processing of complex PDFs \n",
    "- **UnstructuredHTMLLoader**: When web pages are the name of the game\n",
    "- **CSVLoader**: Tabular data? No problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the document loaders - https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n",
    "# AzureAIDocumentIntelligenceLoader warrants further research\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    BSHTMLLoader,\n",
    "    SpiderLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    CSVLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the PyMuPDFLoader for its speed, ability to handle complex PDFs, and more extensive metadata.\n",
    "\n",
    "DOCUMENT_LOADER = PyMuPDFLoader\n",
    "# DOCUMENT_LOADER = \"PyPDFLoader\"\n",
    "# DOCUMENT_LOADER = \"DirectoryLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"BSHTMLLoader\"\n",
    "# DOCUMENT_LOADER = \"SpiderLoader\"\n",
    "# DOCUMENT_LOADER = \"JSONLoader\"\n",
    "# DOCUMENT_LOADER = \"UnstructuredMarkdownLoader\"\n",
    "# DOCUMENT_LOADER = \"CSVLoader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Strategic Text Splitting\n",
    "- **RecursiveCharacterTextSplitter**: The smart way to keep related info together\n",
    "- **TokenTextSplitter**: For when token limits matter most\n",
    "- **HuggingFaceTextSplitter**: Leveraging the best in NLP for optimal splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the text splitters - https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "# info on using HF tokenizers:  https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/#hugging-face-tokenizer\n",
    "# Use of RecursiveCharacterTextSplitter to split code - https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/code_splitter/\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    "    Language,\n",
    ")\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the text splitter to use\n",
    "\n",
    "TEXT_SPLITTER = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )\n",
    "# TEXT_SPLITTER = TokenTextSplitter\n",
    "# TEXT_SPLITTER = MarkdownHeaderTextSplitter\n",
    "# TEXT_SPLITTER = RecursiveJsonSplitter\n",
    "# TEXT_SPLITTER = SemanticChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™¢ Powerful Embeddings\n",
    "- **OpenAIEmbeddings**: Harnessing the power of cutting-edge language models\n",
    "- **CohereEmbeddings**: When diversity and flexibility are key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import embedding models - https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/\n",
    "# note ability to cache embeddings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the embedding model to use\n",
    "EMBEDDING_MODEL = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    "    )\n",
    "# EMBEDDING_MODEL = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Blazing-Fast Vector Stores\n",
    "- **Qdrant**: The high-performance, scalable choice for demanding workloads\n",
    "- **Chroma**: Unbeatable speed and efficiency for real-time use cases\n",
    "- **Pinecone**: Fully-managed simplicity and reliability at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donbr/aie3-bootcamp/AIE3/Week 3/Day 2/.venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import vector stores - https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/\n",
    "# after installing additional python dependencies, I started seeing protobuf errors with the Chroma vector store\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_pinecone import Pinecone\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the vector store to use\n",
    "\n",
    "# VECTOR_STORE = 'Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))'\n",
    "# VECTOR_STORE = Qdrant(\n",
    "#     client=client\n",
    "#     embeddings=EMBEDDING_MODEL,\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "# )\n",
    "# VECTOR_STORE = 'Pinecone()'\n",
    "# VECTOR_STORE = 'FAISS()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Vector Store client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant client instance\n",
    "client = QdrantClient(url=QDRANT_API_URL, api_key=QDRANT_API_KEY, prefer_grpc=True)\n",
    "\n",
    "# Initialize the Qdrant vector store\n",
    "qdrant = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embeddings=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Time for New Docs? Let's Check!\n",
    "\n",
    "The `LOAD_NEW_DATA` flag is a key part of our simple data ingestion pipeline. When set to `True`, it allows the loading of new documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Ingesting Fresh Docs: Embracing Adaptability \n",
    "\n",
    "By using a flag like `LOAD_NEW_DATA`, we can control when new data is ingested without modifying the code itself. This supports rapid experimentation and iteration, as we can test our RAG system with different datasets by simply toggling the flag.\n",
    "\n",
    "In this case, we're using `PyMuPDFLoader` to load a PDF file, but the beauty of this setup is that we can easily switch to other loaders like `UnstructuredHTMLLoader` for HTML files or `CSVLoader` for CSV data by changing the `DOCUMENT_LOADER` variable. This flexibility is crucial for adapting our pipeline to experiment with various data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run loader if LOAD_NEW_DATA is True\n",
    "if LOAD_NEW_DATA:\n",
    "    loader = DOCUMENT_LOADER(FILE_PATH)\n",
    "    docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(docs): 14\n",
      "\n",
      "docs[0].page_content[0:100]:\n",
      "Multi-Head RAG: Solving Multi-Aspect Problems\n",
      "with LLMs\n",
      "Maciej Besta1‚àó\n",
      "Ales Kubicek1\n",
      "Roman Niggli1\n",
      "R\n",
      "\n",
      "docs[0].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 0, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "docs[1].page_content[0:100]:\n",
      "hallucinations (by grounding the LLM reply in reliable sources), and ensuring that responses contain\n",
      "\n",
      "docs[1].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 1, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "docs[-2].page_content[0:100]:\n",
      "[37] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng\n",
      "Wang, and Z\n",
      "\n",
      "docs[-2].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 12, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "docs[-1].page_content[0:100]:\n",
      "Appendix\n",
      "A\n",
      "Model Design: Additional Details\n",
      "A.1\n",
      "Retrieval Strategies for Multi-Aspect Data\n",
      "Algorithm\n",
      "\n",
      "docs[-1].metadata):\n",
      "{'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 13, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# Document Loader validation\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(docs): {len(docs)}\")\n",
    "    print(f\"\\ndocs[0].page_content[0:100]:\\n{docs[0].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[0].metadata):\\n{docs[0].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[1].page_content[0:100]:\\n{docs[1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[1].metadata):\\n{docs[1].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-2].page_content[0:100]:\\n{docs[-2].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-2].metadata):\\n{docs[-2].metadata}\")\n",
    "\n",
    "    print(f\"\\ndocs[-1].page_content[0:100]:\\n{docs[-1].page_content[0:100]}\")\n",
    "    print(f\"\\ndocs[-1].metadata):\\n{docs[-1].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Intelligent Text Splitting\n",
    "\n",
    "Once our data is loaded, the next step is splitting it into manageable chunks. We're using the `RecursiveCharacterTextSplitter` for this, which intelligently splits text while keeping related pieces together.\n",
    "\n",
    "The splitter works by recursively dividing the text on specified characters (like newlines and periods) until each chunk is within our desired `chunk_size`. The `chunk_overlap` parameter ensures some overlap between chunks to maintain context.\n",
    "\n",
    "By adjusting these parameters, we can fine-tune the output to suit our specific use case. For example, a larger `chunk_size` results in fewer, longer chunks, while more `chunk_overlap` helps preserve context across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_NEW_DATA:\n",
    "    text_splitter = TEXT_SPLITTER\n",
    "    splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(splits): 69\n",
      "\n",
      "splits[0]:\n",
      "page_content='Multi-Head RAG: Solving Multi-Aspect Problems\\nwith LLMs\\nMaciej Besta1‚àó\\nAles Kubicek1\\nRoman Niggli1\\nRobert Gerstenberger1\\nLucas Weitzendorf1\\nMingyuan Chi1\\nPatrick Iff1\\nJoanna Gajda2\\nPiotr Nyczyk2\\nJ√ºrgen M√ºller3\\nHubert Niewiadomski2\\nMarcin Chrapek1\\nMicha≈Ç Podstawski4\\nTorsten Hoefler1\\n1ETH Zurich\\n2Cledar\\n3BASF SE\\n4Warsaw University of Technology\\nAbstract\\nRetrieval Augmented Generation (RAG) enhances the abilities of Large Language\\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\\nprovide more accurate and relevant responses. Existing RAG solutions do not\\nfocus on queries that may require fetching multiple documents with substantially\\ndifferent contents. Such queries occur frequently, but are challenging because the\\nembeddings of these documents may be distant in the embedding space, making it\\nhard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel\\nscheme designed to address this gap with a simple yet powerful idea: leveraging' metadata={'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 0, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "splits[1]:\n",
      "page_content='hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel\\nscheme designed to address this gap with a simple yet powerful idea: leveraging\\nactivations of Transformer‚Äôs multi-head attention layer, instead of the decoder\\nlayer, as keys for fetching multi-aspect documents. The driving motivation is that\\ndifferent attention heads can learn to capture different data aspects. Harnessing the\\ncorresponding activations results in embeddings that represent various facets of\\ndata items and queries, improving the retrieval accuracy for complex queries. We\\nprovide an evaluation methodology and metrics, synthetic datasets, and real-world\\nuse cases to demonstrate MRAG‚Äôs effectiveness, showing improvements of up\\nto 20% in relevance over standard RAG baselines. MRAG can be seamlessly\\nintegrated with existing RAG frameworks and benchmarking tools like RAGAS as\\nwell as different classes of data stores.\\nWebsite & code: https://github.com/spcl/MRAG\\n1\\nIntroduction' metadata={'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 0, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "splits[-2]:\n",
      "page_content='Appendix\\nA\\nModel Design: Additional Details\\nA.1\\nRetrieval Strategies for Multi-Aspect Data\\nAlgorithm 2 Voting strategy.\\nl ‚Üê[]\\nfor each head hi and its score si do\\nfind best matching k text chunks\\nfor each chunk di,p with index p in top k\\ndo\\nwi,p ‚Üêsi ¬∑ 2‚àíp\\nadd tuple (di,p, wi,p) to l\\nend for\\nend for\\nsort l using weights wi,p; return top k elems\\nB\\nEvaluation Methodology: Additional Details\\nB.1\\nCompute Resources\\nOur experiments were executed with compute nodes containing 4x NVIDIA GH200 and a total\\nmemory of 800 GB. In general one GPU with at least 40GB of memory should suffice. We used\\nat most 50GB of storage and the OpenAI API as an external resource. The full experiments took\\nat most three hours of GPU time and the cost for the OpenAI API were at most $15. We carried\\nout additional experiments, which amounted to around 20 hours of GPU time and cost of $25 for\\nthe OpenAI API. Additional evaluation was executed with a mix of compute resources including\\nNVIDIA A100 and V100 GPUs.\\nB.2' metadata={'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 13, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "splits[-1]:\n",
      "page_content='the OpenAI API. Additional evaluation was executed with a mix of compute resources including\\nNVIDIA A100 and V100 GPUs.\\nB.2\\nDataset Details\\nTable 2: Prompt template for query generation.\\nPlease create a story about the attached <number of articles> articles on the topics <list of titles>.\\nIt is very important that each of the attached articles is relevant to the story, in a way that references\\nthe content of the article, not just its title. But please also mention each title at least once. Please\\nmake sure that all of the attached articles are relevant to your story, and that each article is referenced\\nin at least two sentences! They do not necessarily have to be referenced in the same order, but make\\nsure no article is forgotten.\\nImportant: Output only the story, no additional text. And do not use bullet points, or paragraphs.\\nArticles:\\n‚Äî‚Äî‚Äî\\nArticle <title>:\\n<body>\\n<...>\\n‚Äî‚Äî‚Äî\\nAgain, make sure that you reference all the following topics in your story: <list of titles>\\n14' metadata={'source': 'https://arxiv.org/pdf/2406.05085', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'page': 13, 'total_pages': 14, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240610005547Z', 'modDate': 'D:20240610005547Z', 'trapped': ''}\n",
      "\n",
      "Split # 0:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[0]): 987\n",
      "splits[0][0:25]: Multi-Head RAG: Solving M\n",
      "\n",
      "Split # 1:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[1]): 977\n",
      "splits[1][0:25]: hard to retrieve them all\n",
      "\n",
      "Split # 2:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[2]): 951\n",
      "splits[2][0:25]: integrated with existing \n",
      "\n",
      "Split # 3:\n",
      "split.metadata.get('page'): 0\n",
      "len(splits[3]): 790\n",
      "splits[3][0:25]: 38, 44] by providing fact\n",
      "\n",
      "Split # 4:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[4]): 965\n",
      "splits[4][0:25]: hallucinations (by ground\n",
      "\n",
      "Split # 5:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[5]): 956\n",
      "splits[5][0:25]: retrieved data more accur\n",
      "\n",
      "Split # 6:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[6]): 964\n",
      "splits[6][0:25]: Alexander the Great and o\n",
      "\n",
      "Split # 7:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[7]): 990\n",
      "splits[7][0:25]: old?‚Äù), maintenance (‚ÄúWas\n",
      "\n",
      "Split # 8:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[8]): 980\n",
      "splits[8][0:25]: Figure 1: An overview of \n",
      "\n",
      "Split # 9:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[9]): 938\n",
      "splits[9][0:25]: It is conjectured that th\n",
      "\n",
      "Split # 10:\n",
      "split.metadata.get('page'): 1\n",
      "len(splits[10]): 289\n",
      "splits[10][0:25]: embedding idea (contribut\n",
      "\n",
      "Split # 11:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[11]): 979\n",
      "splits[11][0:25]: of retrieved documents, f\n",
      "\n",
      "Split # 12:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[12]): 918\n",
      "splits[12][0:25]: \u0010\u0000qh\n",
      "i\n",
      "\u0001T kh\n",
      "j\n",
      "\u0011\n",
      ", qh\n",
      "i =\n",
      "\n",
      "Split # 13:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[13]): 942\n",
      "splits[13][0:25]: last nth token of this ch\n",
      "\n",
      "Split # 14:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[14]): 918\n",
      "splits[14][0:25]: mixing with Wo, we conjec\n",
      "\n",
      "Split # 15:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[15]): 963\n",
      "splits[15][0:25]: Data Preparation\n",
      "During d\n",
      "\n",
      "Split # 16:\n",
      "split.metadata.get('page'): 2\n",
      "len(splits[16]): 492\n",
      "splits[16][0:25]: MRAG stores data differen\n",
      "\n",
      "Split # 17:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[17]): 998\n",
      "splits[17][0:25]: Source documents\n",
      "Text chu\n",
      "\n",
      "Split # 18:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[18]): 991\n",
      "splits[18][0:25]: two aspects (     ),\n",
      "one \n",
      "\n",
      "Split # 19:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[19]): 953\n",
      "splits[19][0:25]: embeddings\n",
      "and their corr\n",
      "\n",
      "Split # 20:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[20]): 933\n",
      "splits[20][0:25]: aspect embeddings for a g\n",
      "\n",
      "Split # 21:\n",
      "split.metadata.get('page'): 3\n",
      "len(splits[21]): 663\n",
      "splits[21][0:25]: embedding of the user que\n",
      "\n",
      "Split # 22:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[22]): 971\n",
      "splits[22][0:25]: Algorithm 1 Importance sc\n",
      "\n",
      "Split # 23:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[23]): 982\n",
      "splits[23][0:25]: tention head. bi is the a\n",
      "\n",
      "Split # 24:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[24]): 922\n",
      "splits[24][0:25]: is in the Appendix). Each\n",
      "\n",
      "Split # 25:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[25]): 943\n",
      "splits[25][0:25]: orthogonal to MRAG.\n",
      "3\n",
      "Mul\n",
      "\n",
      "Split # 26:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[26]): 977\n",
      "splits[26][0:25]: board games, historical s\n",
      "\n",
      "Split # 27:\n",
      "split.metadata.get('page'): 4\n",
      "len(splits[27]): 607\n",
      "splits[27][0:25]: requires retrieving 10 do\n",
      "\n",
      "Split # 28:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[28]): 994\n",
      "splits[28][0:25]: In a realm where the digi\n",
      "\n",
      "Split # 29:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[29]): 942\n",
      "splits[29][0:25]: In a realm where the echo\n",
      "\n",
      "Split # 30:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[30]): 992\n",
      "splits[30][0:25]: whimsical story transport\n",
      "\n",
      "Split # 31:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[31]): 979\n",
      "splits[31][0:25]: Celso Daniel\n",
      "James and th\n",
      "\n",
      "Split # 32:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[32]): 957\n",
      "splits[32][0:25]: 2/10\n",
      "3/10\n",
      "Weighted Retrie\n",
      "\n",
      "Split # 33:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[33]): 954\n",
      "splits[33][0:25]: success ratio of differen\n",
      "\n",
      "Split # 34:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[34]): 928\n",
      "splits[34][0:25]: the ideal desired documen\n",
      "\n",
      "Split # 35:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[35]): 932\n",
      "splits[35][0:25]: Specifically, it splits t\n",
      "\n",
      "Split # 36:\n",
      "split.metadata.get('page'): 5\n",
      "len(splits[36]): 571\n",
      "splits[36][0:25]: a specific number of docu\n",
      "\n",
      "Split # 37:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[37]): 936\n",
      "splits[37][0:25]: Figure 4: Retrieval succe\n",
      "\n",
      "Split # 38:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[38]): 945\n",
      "splits[38][0:25]: MRAG over Standard RAG in\n",
      "\n",
      "Split # 39:\n",
      "split.metadata.get('page'): 6\n",
      "len(splits[39]): 741\n",
      "splits[39][0:25]: Standard RAG as we vary t\n",
      "\n",
      "Split # 40:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[40]): 916\n",
      "splits[40][0:25]: Figure 6: Relative retrie\n",
      "\n",
      "Split # 41:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[41]): 914\n",
      "splits[41][0:25]: more accurate retrieval. \n",
      "\n",
      "Split # 42:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[42]): 982\n",
      "splits[42][0:25]: We also compare MRAG to t\n",
      "\n",
      "Split # 43:\n",
      "split.metadata.get('page'): 7\n",
      "len(splits[43]): 911\n",
      "splits[43][0:25]: style of the document (e.\n",
      "\n",
      "Split # 44:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[44]): 952\n",
      "splits[44][0:25]: Figure 8: Evaluation of d\n",
      "\n",
      "Split # 45:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[45]): 937\n",
      "splits[45][0:25]: for MRAG. We compare MRAG\n",
      "\n",
      "Split # 46:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[46]): 927\n",
      "splits[46][0:25]: are on-par with each othe\n",
      "\n",
      "Split # 47:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[47]): 914\n",
      "splits[47][0:25]: in conjunction with such \n",
      "\n",
      "Split # 48:\n",
      "split.metadata.get('page'): 8\n",
      "len(splits[48]): 604\n",
      "splits[48][0:25]: relationships or the inne\n",
      "\n",
      "Split # 49:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[49]): 927\n",
      "splits[49][0:25]: documents with significan\n",
      "\n",
      "Split # 50:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[50]): 961\n",
      "splits[50][0:25]: complex, multi-aspect que\n",
      "\n",
      "Split # 51:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[51]): 916\n",
      "splits[51][0:25]: embedding model. This eff\n",
      "\n",
      "Split # 52:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[52]): 986\n",
      "splits[52][0:25]: Computing Joint Undertaki\n",
      "\n",
      "Split # 53:\n",
      "split.metadata.get('page'): 9\n",
      "len(splits[53]): 976\n",
      "splits[53][0:25]: Learning to Retrieve, Gen\n",
      "\n",
      "Split # 54:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[54]): 928\n",
      "splits[54][0:25]: [7] Darren Edge, Ha Trinh\n",
      "\n",
      "Split # 55:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[55]): 982\n",
      "splits[55][0:25]: Meng Wang, and Haofen Wan\n",
      "\n",
      "Split # 56:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[56]): 935\n",
      "splits[56][0:25]: for Large Language Models\n",
      "\n",
      "Split # 57:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[57]): 990\n",
      "splits[57][0:25]: arXiv:2312.15883\n",
      "[18] Pat\n",
      "\n",
      "Split # 58:\n",
      "split.metadata.get('page'): 10\n",
      "len(splits[58]): 447\n",
      "splits[58][0:25]: [20] Huayang Li, Yixuan S\n",
      "\n",
      "Split # 59:\n",
      "split.metadata.get('page'): 11\n",
      "len(splits[59]): 952\n",
      "splits[59][0:25]: [22] Sean MacAvaney, Andr\n",
      "\n",
      "Split # 60:\n",
      "split.metadata.get('page'): 11\n",
      "len(splits[60]): 988\n",
      "splits[60][0:25]: AI Research Blog. https:/\n",
      "\n",
      "Split # 61:\n",
      "split.metadata.get('page'): 11\n",
      "len(splits[61]): 996\n",
      "splits[61][0:25]: From LLMs? Objectives for\n",
      "\n",
      "Split # 62:\n",
      "split.metadata.get('page'): 11\n",
      "len(splits[62]): 960\n",
      "splits[62][0:25]: ≈Åukasz Kaiser, and Illia \n",
      "\n",
      "Split # 63:\n",
      "split.metadata.get('page'): 11\n",
      "len(splits[63]): 308\n",
      "splits[63][0:25]: training. arXiv:2212.0353\n",
      "\n",
      "Split # 64:\n",
      "split.metadata.get('page'): 12\n",
      "len(splits[64]): 992\n",
      "splits[64][0:25]: [37] Zhentao Xu, Mark Jer\n",
      "\n",
      "Split # 65:\n",
      "split.metadata.get('page'): 12\n",
      "len(splits[65]): 989\n",
      "splits[65][0:25]: Learning. arXiv:2402.1354\n",
      "\n",
      "Split # 66:\n",
      "split.metadata.get('page'): 12\n",
      "len(splits[66]): 426\n",
      "splits[66][0:25]: Zhao, Yu Zhang, Yulong Ch\n",
      "\n",
      "Split # 67:\n",
      "split.metadata.get('page'): 13\n",
      "len(splits[67]): 995\n",
      "splits[67][0:25]: Appendix\n",
      "A\n",
      "Model Design: \n",
      "\n",
      "Split # 68:\n",
      "split.metadata.get('page'): 13\n",
      "len(splits[68]): 983\n",
      "splits[68][0:25]: the OpenAI API. Additiona\n"
     ]
    }
   ],
   "source": [
    "# capture the split chunks for use in the vector store\n",
    "if LOAD_NEW_DATA:\n",
    "    print(f\"len(splits): {len(splits)}\")\n",
    "\n",
    "    print(f\"\\nsplits[0]:\\n{splits[0]}\")\n",
    "    print(f\"\\nsplits[1]:\\n{splits[1]}\")\n",
    "    print(f\"\\nsplits[-2]:\\n{splits[-2]}\")\n",
    "    print(f\"\\nsplits[-1]:\\n{splits[-1]}\")\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        print(f\"\\nSplit # {i}:\")\n",
    "        # print page number from split.metadata\n",
    "\n",
    "        print(f\"split.metadata.get('page'): {split.metadata.get('page')}\")\n",
    "        print(f\"len(splits[{i}]): {len(split.page_content)}\")\n",
    "        print(f\"splits[{i}][0:25]: {split.page_content[0:25]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Supercharging Our RAG System with Qdrant\n",
    "\n",
    "With our text split into manageable chunks, it's time to vectorize and store them for fast retrieval. That's where Qdrant comes in - a state-of-the-art vector database that offers unparalleled performance, scalability, and flexibility.\n",
    "\n",
    "Qdrant utilizes the HNSW algorithm for blazing-fast similarity search, delivering up to 4x higher requests per second compared to alternatives. Its advanced compression features reduce memory usage by up to 97%, while its flexible storage options allow us to fine-tune for our specific needs.\n",
    "\n",
    "But Qdrant isn't just fast - it's also incredibly versatile. With support for hybrid search (combining vector similarity and filtering), sparse vectors, and rich JSON payloads, Qdrant enables powerful querying patterns that go beyond simple similarity search.\n",
    "\n",
    "And with a robust set of enterprise features like multitenancy, access control, and backup/recovery, Qdrant is ready to scale with our RAG system as it grows.\n",
    "\n",
    "By leveraging Qdrant's speed, efficiency, and flexibility, we're building a knowledge base that can rapidly retrieve the most relevant information for any query. Whether we're serving a small prototype or a massive production system, Qdrant has us covered.\n",
    "\n",
    "So let's dive in and see how Qdrant can supercharge our RAG system! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the chunks in Qdrant\n",
    "if LOAD_NEW_DATA:\n",
    "    from_splits = qdrant.from_documents(\n",
    "        url=QDRANT_API_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "        prefer_grpc=True,\n",
    "        documents=splits,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=EMBEDDING_MODEL\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Implementing a Robust Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Constructing the RAG Chain for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": LLAMA3_PROMPT | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      +---------------------------------+                        \n",
      "                      | Parallel<context,question>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                           ****                   ****                           \n",
      "                       ****                           ***                        \n",
      "                     **                                  ****                    \n",
      "+--------------------------------+                           **                  \n",
      "| Lambda(itemgetter('question')) |                            *                  \n",
      "+--------------------------------+                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "                 *                                            *                  \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "     | VectorStoreRetriever |                 | Lambda(itemgetter('question')) | \n",
      "     +----------------------+                 +--------------------------------+ \n",
      "                           ****                   ****                           \n",
      "                               ****           ****                               \n",
      "                                   **       **                                   \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<context,question>Output |                       \n",
      "                      +----------------------------------+                       \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                           +------------------------+                            \n",
      "                           | Parallel<context>Input |                            \n",
      "                           +------------------------+                            \n",
      "                              ***               ***                              \n",
      "                           ***                     ***                           \n",
      "                         **                           **                         \n",
      "     +-------------------------------+            +-------------+                \n",
      "     | Lambda(itemgetter('context')) |            | Passthrough |                \n",
      "     +-------------------------------+            +-------------+                \n",
      "                              ***               ***                              \n",
      "                                 ***         ***                                 \n",
      "                                    **     **                                    \n",
      "                          +-------------------------+                            \n",
      "                          | Parallel<context>Output |                            \n",
      "                          +-------------------------+                            \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                                        *                                        \n",
      "                      +---------------------------------+                        \n",
      "                      | Parallel<response,context>Input |                        \n",
      "                      +---------------------------------+                        \n",
      "                            ****                 ***                             \n",
      "                         ***                        ***                          \n",
      "                       **                              ****                      \n",
      "         +--------------------+                            **                    \n",
      "         | ChatPromptTemplate |                             *                    \n",
      "         +--------------------+                             *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "                    *                                       *                    \n",
      "              +----------+                 +-------------------------------+     \n",
      "              | ChatGroq |                 | Lambda(itemgetter('context')) |     \n",
      "              +----------+**               +-------------------------------+     \n",
      "                            ****                ****                             \n",
      "                                ***          ***                                 \n",
      "                                   **      **                                    \n",
      "                      +----------------------------------+                       \n",
      "                      | Parallel<response,context>Output |                       \n",
      "                      +----------------------------------+                       \n"
     ]
    }
   ],
   "source": [
    "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Moment of Truth: Testing Our RAG System!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"question\" : QUESTION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a step-by-step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions:\\n\\n1. Identify the evaluation goals: Determine what aspects of the RAG system need to be evaluated, such as the retrieval model, the language model, and the overall system performance.\\n2. Choose evaluation metrics: Select relevant metrics that align with the evaluation goals, such as perplexity, accuracy, fluency, and relevance.\\n3. Develop a reference-free evaluation framework: Utilize a framework like RAGAS, which provides a comprehensive and automated approach to evaluating RAG systems without relying on reference outputs.\\n4. Consider multiple dimensions: Evaluate the RAG system across multiple dimensions, including the retrieval model's ability to identify relevant context passages, the language model's ability to exploit those passages, and the system's overall performance.\\n5. Perform automated evaluation: Leverage automated evaluation techniques to assess the RAG system's performance on various tasks, such as question answering, text generation, and conversation.\\n6. Validate results: Validate the evaluation results by comparing them with human evaluations or other automated evaluation methods to ensure the reliability and accuracy of the assessment.\\n\\nBy following these steps, companies can establish a robust approach to evaluating RAG solutions, ensuring that their systems are accurately assessed and improved.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the response.  filter on the response key AIMessage content element\n",
    "response[\"response\"].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a step-by-step plan to guide companies in establishing a robust approach to evaluating Retrieval Augmented Generation (RAG) solutions:\n",
      "\n",
      "1. Identify the evaluation goals: Determine what aspects of the RAG system need to be evaluated, such as the retrieval model, the language model, and the overall system performance.\n",
      "2. Choose evaluation metrics: Select relevant metrics that align with the evaluation goals, such as perplexity, accuracy, fluency, and relevance.\n",
      "3. Develop a reference-free evaluation framework: Utilize a framework like RAGAS, which provides a comprehensive and automated approach to evaluating RAG systems without relying on reference outputs.\n",
      "4. Consider multiple dimensions: Evaluate the RAG system across multiple dimensions, including the retrieval model's ability to identify relevant context passages, the language model's ability to exploit those passages, and the system's overall performance.\n",
      "5. Perform automated evaluation: Leverage automated evaluation techniques to assess the RAG system's performance on various tasks, such as question answering, text generation, and conversation.\n",
      "6. Validate results: Validate the evaluation results by comparing them with human evaluations or other automated evaluation methods to ensure the reliability and accuracy of the assessment.\n",
      "\n",
      "By following these steps, companies can establish a robust approach to evaluating RAG solutions, ensuring that their systems are accurately assessed and improved.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vector store CONTEXT # 0:\n",
      "Page # : 0\n",
      "context.page_content:\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es‚Ä†, Jithin James‚Ä†, Luis Espinosa-Anke‚àó‚ô¢, Steven Schockaert‚àó\n",
      "‚Ä†Exploding Gradients\n",
      "‚àóCardiffNLP, Cardiff University, United Kingdom\n",
      "‚ô¢AMPLYFI, United Kingdom\n",
      "shahules786@gmail.com,jamesjithin97@gmail.com\n",
      "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "Abstract\n",
      "We introduce RAGAS (Retrieval Augmented\n",
      "Generation Assessment), a framework for\n",
      "reference-free evaluation of Retrieval Aug-\n",
      "mented Generation (RAG) pipelines.\n",
      "RAG\n",
      "systems are composed of a retrieval and an\n",
      "LLM based generation module, and provide\n",
      "LLMs with knowledge from a reference textual\n",
      "database, which enables them to act as a natu-\n",
      "ral language layer between a user and textual\n",
      "databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal-\n",
      "lenging because there are several dimensions to\n",
      "consider: the ability of the retrieval system to\n",
      "identify relevant and focused context passages,\n",
      "the ability of the LLM to exploit such passages\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 8, 'keywords': '', 'trapped': '', 'modDate': 'D:20230928011700Z', 'format': 'PDF 1.5', 'creationDate': 'D:20230928011700Z', 'file_path': 'https://arxiv.org/pdf/2309.15217', 'source': 'https://arxiv.org/pdf/2309.15217', 'title': '', 'page': 0, 'producer': 'pdfTeX-1.40.25', 'author': '', '_id': '198744fc-4ae2-4a29-a2d8-5ddd67fc584a', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 1:\n",
      "Page # : 0\n",
      "context.page_content:\n",
      "a significant amount of tuning, as the overall per-\n",
      "formance will be affected by the retrieval model,\n",
      "the considered corpus, the LM, or the prompt for-\n",
      "mulation, among others. Automated evaluation of\n",
      "retrieval-augmented systems is thus paramount. In\n",
      "practice, RAG systems are often evaluated in terms\n",
      "of the language modelling task itself, i.e. by mea-\n",
      "suring perplexity on some reference corpus. How-\n",
      "ever, such evaluations are not always predictive\n",
      "of downstream performance (Wang et al., 2023c).\n",
      "Moreover, this evaluation strategy relies on the LM\n",
      "probabilities, which are not accessible for some\n",
      "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
      "tion answering is another common evaluation task,\n",
      "but usually only datasets with short extractive an-\n",
      "swers are considered, which may not be represen-\n",
      "tative of how the system will be used.\n",
      "To address these issues, in this paper we present\n",
      "RAGAS1, a framework for the automated assess-\n",
      "1RAGAS\n",
      "is\n",
      "available\n",
      "at\n",
      "https://github.com/\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 8, 'keywords': '', 'modDate': 'D:20230928011700Z', 'trapped': '', 'format': 'PDF 1.5', 'creationDate': 'D:20230928011700Z', 'source': 'https://arxiv.org/pdf/2309.15217', 'file_path': 'https://arxiv.org/pdf/2309.15217', 'title': '', 'page': 0, 'producer': 'pdfTeX-1.40.25', 'author': '', '_id': 'ec239e82-35d6-49c7-9ddd-30cfe3eaffd8', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 2:\n",
      "Page # : 1\n",
      "context.page_content:\n",
      "hallucinations (by grounding the LLM reply in reliable sources), and ensuring that responses contain\n",
      "up-to-date knowledge (e.g., by accessing the Internet), all without requiring expensive training.\n",
      "More specifically, there are two main stages in a RAG pipeline: data preparation and query execution.\n",
      "During data preparation, one constructs a vector database (DB) populated with embeddings and their\n",
      "corresponding data items such as documents. During query execution, one constructs an embedding\n",
      "of that query and retrieves data items in the store with similar embeddings.\n",
      "Intense recent research efforts have been put into RAG [10, 12, 14, 20, 25, 41, 45]. On one hand,\n",
      "different RAG designs have been proposed, for example RAPTOR [31], Self-RAG [2], Chain-of-\n",
      "Note [42], and many others [1, 6, 7, 23, 35, 39, 43]. In general, these schemes focus on making the\n",
      "retrieved data more accurate and relevant to the query. On the other hand, there have also been efforts\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 14, 'keywords': '', 'modDate': 'D:20240610005547Z', 'trapped': '', 'format': 'PDF 1.5', 'source': 'https://arxiv.org/pdf/2406.05085', 'creationDate': 'D:20240610005547Z', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'producer': 'pdfTeX-1.40.25', 'page': 1, 'title': '', 'author': '', '_id': '60be6ace-069f-4fd0-908f-b63731842460', '_collection_name': 'rag_evaluation'}\n",
      "\n",
      "vector store CONTEXT # 3:\n",
      "Page # : 8\n",
      "context.page_content:\n",
      "relationships or the inner organization of text. Usually, they need a sophisticated preprocessing phase\n",
      "to prepare such structures. MRAG achieves the improvement solely based on the embedding model\n",
      "and has no additional storage requirements, and can be combined with any of these schemes.\n",
      "6\n",
      "Conclusion\n",
      "Retrieval Augmented Generation (RAG) is pivotal for democratizing access to accurate and relevant\n",
      "outputs from large language models (LLMs). Enhancing the precision and relevance of these outputs\n",
      "is a critical goal, especially given the challenges posed by queries requiring the retrieval of multiple\n",
      "9\n",
      "context.metadata:\n",
      "{'subject': '', 'creator': 'LaTeX with hyperref', 'total_pages': 14, 'keywords': '', 'modDate': 'D:20240610005547Z', 'trapped': '', 'format': 'PDF 1.5', 'file_path': 'https://arxiv.org/pdf/2406.05085', 'source': 'https://arxiv.org/pdf/2406.05085', 'creationDate': 'D:20240610005547Z', 'producer': 'pdfTeX-1.40.25', 'page': 8, 'title': '', 'author': '', '_id': 'd7f9d8c3-b3a1-4fe3-a3b9-d41d5686985e', '_collection_name': 'rag_evaluation'}\n"
     ]
    }
   ],
   "source": [
    "for i, context_instance in enumerate(response[\"context\"]):\n",
    "  print(f\"\\nvector store CONTEXT # {i}:\")\n",
    "  print(f\"Page # : {context_instance.metadata.get('page')}\")\n",
    "  print(f\"context.page_content:\\n{context_instance.page_content}\")\n",
    "  print(f\"context.metadata:\\n{context_instance.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
