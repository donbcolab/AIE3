{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LangChain v0.2.0 and LCEL: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Initialize a Simple Chain using LCEL\n",
        "  4. Implement Naive RAG using LCEL\n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Create a Simple RAG Application Using QDrant, OpenAI, and LCEL\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVHd6POM0JFN"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "One of the [key features](https://blog.langchain.dev/langchain-v02-leap-to-stability/) of LangChain v0.2.0 is the compartmentalization of the various LangChain ecosystem packages and added stability.\n",
        "\n",
        "Instead of one all encompassing Python package - LangChain has a `core` package and a number of additional supplementary packages.\n",
        "\n",
        "We'll start by grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCC2AR-Q0m0x",
        "outputId": "a9936260-6134-4294-f21c-6a40111a57ca"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-core langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ELHQjQ1PYs"
      },
      "source": [
        "Now we can get our Qdrant dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XeYI9P1OXO",
        "outputId": "20cf9b81-95bd-4ebf-ae3a-cf0f5912f3eb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iesey9OGCKJx"
      },
      "source": [
        "Let's finally get `tiktoken` and `pymupdf` so we can leverage them later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5qIUrFuENrS",
        "outputId": "61571142-eb50-4eaa-ac68-4c39062a36ad"
      },
      "outputs": [],
      "source": [
        "!pip install -qU tiktoken pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll be leveraging OpenAI's suite of APIs - so we'll set our `OPENAI_API_KEY` `env` variable here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pKAfycq73wE",
        "outputId": "404522d0-c907-44bc-c8f8-f72084dfbb57"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_xp54wIA56_"
      },
      "source": [
        "## Task 3: Initialize a Simple Chain using LCEL\n",
        "\n",
        "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyGdhbS6SkD1"
      },
      "source": [
        "### LLM Orchestration Tool (LangChain)\n",
        "\n",
        "Let's dive right into [LangChain](https://www.langchain.com/)!\n",
        "\n",
        "The first thing we want to do is create an object that lets us access OpenAI's `gpt-40` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Uj6SorxMj8e"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsmiieEh_Ye-"
      },
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "What other models could we use, and how would the above code change?\n",
        "\n",
        "> HINT: Check out [this page](https://platform.openai.com/docs/models/gpt-3-5-turbo) to find the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  Update the model parameter when calling the `ChatOpenAI` class\n",
        "\n",
        "Here are a few examples of other models you could use and how the Python code would change accordingly:\n",
        "\n",
        "1. **GPT-3.5 Turbo**:\n",
        "   ```python\n",
        "   from langchain_openai import ChatOpenAI\n",
        "\n",
        "   openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "   ```\n",
        "\n",
        "2. **GPT-3.5**:\n",
        "   ```python\n",
        "   from langchain_openai import ChatOpenAI\n",
        "\n",
        "   openai_chat_model = ChatOpenAI(model=\"gpt-3.5\")\n",
        "   ```\n",
        "\n",
        "5. **GPT-4**:\n",
        "   ```python\n",
        "   from langchain_openai import ChatOpenAI\n",
        "\n",
        "   openai_chat_model = ChatOpenAI(model=\"gpt-4\")\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Chat Model gpt-3.5-turbo is supported.\n",
            "OpenAI Chat Model gpt-3.5 is supported.\n",
            "OpenAI Chat Model gpt-4 is supported.\n",
            "OpenAI Chat Model text-davinci-003 is supported.\n",
            "OpenAI Chat Model code-davinci-002 is supported.\n",
            "OpenAI Chat Model gpt-3.5-turbo-0301 is supported.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def check_supported_models(model_versions):\n",
        "    for model in model_versions:\n",
        "        try:\n",
        "            openai_chat_model = ChatOpenAI(model=model)\n",
        "            print(f\"OpenAI Chat Model {model} is supported.\")\n",
        "        except Exception as e:\n",
        "            print(f\"OpenAI Chat Model {model} is not supported. Error: {e}\")\n",
        "\n",
        "# List of model versions to check\n",
        "model_versions = [\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-3.5\",\n",
        "    \"gpt-4\",\n",
        "    \"text-davinci-003\",\n",
        "    \"code-davinci-002\",\n",
        "    \"gpt-3.5-turbo-0301\",\n",
        "]\n",
        "\n",
        "check_supported_models(model_versions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nU8SlHfH41T"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcMKLZWBVYm7"
      },
      "source": [
        "Now, we'll set up a prompt template - more specifically a `ChatPromptTemplate`. This will let us build a prompt we can modify when we call our LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Z770j4zPS3o5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['content'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['content'], template='{content}'))])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGku_c2VVyd_"
      },
      "source": [
        "### Our First Chain\n",
        "\n",
        "Now we can set up our first chain!\n",
        "\n",
        "A chain is simply two components that feed directly into eachother in a sequential fashion!\n",
        "\n",
        "You'll notice that we're using the pipe operator `|` to connect our `chat_prompt` to our `llm`.\n",
        "\n",
        "This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.\n",
        "\n",
        "You can read more about it [here](https://python.langchain.com/docs/expression_language/), but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):\n",
        "\n",
        "- **Async, Batch, and Streaming Support** Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "- **Fallbacks** The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "- **Parallelism** Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "In the following code cell we have two components:\n",
        "\n",
        "- `chat_prompt`, which is a formattable `ChatPromptTemplate` that contains a system message and a human message.\n",
        "\n",
        "We'd like to be able to pass our own `content` (as found in our `human_template`) and then have the resulting message pair sent to our model and responded to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RcJyqOiwVt04"
      },
      "outputs": [],
      "source": [
        "chain = chat_prompt | openai_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_kHCjlL_01"
      },
      "source": [
        "Notice the pattern here:\n",
        "\n",
        "We invoke our chain with the `dict` `{\"content\" : \"Hello world!\"}`.\n",
        "\n",
        "It enters our chain:\n",
        "\n",
        "`{\"content\" : \"Hello world!\"}` -> `invoke()` -> `chat_prompt`\n",
        "\n",
        "Our `chat_prompt` returns a `PromptValue`, which is the formatted prompt. We then \"pipe\" the output of our `chat_prompt` into our `llm`.\n",
        "\n",
        "`PromptValue` -> `|` -> `llm`\n",
        "\n",
        "Our `llm` then takes the list of messages and provides an output which is return as a `str`!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqr2QuMtIjn",
        "outputId": "7c8655d5-c39e-42dd-8766-c784026b62ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Ah, greetings, seeker of the digital dawn! As the great wheel of destiny turns, so too does your journey commence. Tell me, what curdles your curiosity today? Might it be the riddles of the moon or the mysteries of Manchego?' response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 38, 'total_tokens': 90}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-f8642f72-5b10-4fae-a451-eb9d28c94c7a-0' usage_metadata={'input_tokens': 38, 'output_tokens': 52, 'total_tokens': 90}\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"content\": \"Hello world!\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2znL48ECNteM"
      },
      "source": [
        "Let's try it out with a different prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjiTNeYXUCAB",
        "outputId": "840e9d31-7387-48f3-cbb9-7a3495985698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Ah, seeker of serpentine wisdom, you wish to ascend the peaks of Pythonic prowess! To weave your spells of code like the finest Emmental, you must first embrace the holes—nay, the gaps—in your knowledge. Here are some enchanted morsels for your journey:\\n\\n1. **Embrace the Gouda Foundations:**\\n   Grasp the basics with the tenacity of a wizard clutching his staff. Variables, loops, and functions are the Gruyère upon which you shall build your castle.\\n\\n2. **The Mozzarella of Mastery:**\\n   Melt your mind into the intricacies of data structures. Lists, dictionaries, and sets are the sinews and bones of your conjurations.\\n\\n3. **Brie-ware of the Libraries:**\\n   Seek out arcane tomes such as NumPy, Pandas, and Matplotlib. They hold the power to transform mundane data into a symphony of insights.\\n\\n4. **Cheddar Your Code:**\\n   Let your syntax be as sharp and clean as aged cheddar. Use meaningful names, proper indentation, and comments to make your code as readable as a tale in parchment.\\n\\n5. **Provolone in Practice:**\\n   Engage in quests and challenges. Websites like LeetCode and HackerRank will test your mettle and forge your skills in the fires of competition.\\n\\n6. **The Feta of Feedback:**\\n   Join a guild of fellow programmers. Share your scrolls of code and receive wisdom from the elders. GitHub and Stack Overflow are your portals to this camaraderie.\\n\\n7. **Camembert of Continuation:**\\n   Magic is ever-evolving, and so must you. Keep abreast of the latest spells and incantations in Python by following blogs, attending webinars, and reading the tome of PEP (Python Enhancement Proposals).\\n\\n8. **Havarti a Mentor:**\\n   Seek the counsel of a seasoned sage—a mentor who can guide you through the labyrinthine passages of your learning journey.\\n\\nRemember, young apprentice, the journey of a thousand lines of code begins with but a single import statement. Now go forth and let your Pythonic prowess be as renowned as the finest Roquefort!', response_metadata={'token_usage': {'completion_tokens': 448, 'prompt_tokens': 50, 'total_tokens': 498}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-93fa8f5c-7367-41a1-a123-abbd3ed01bff-0', usage_metadata={'input_tokens': 50, 'output_tokens': 448, 'total_tokens': 498})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"content\" : \"Could I please have some advice on how to become a better Python Programmer?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THcMz8YAWsjP"
      },
      "source": [
        "Notice how we specifically referenced our `content` format option!\n",
        "\n",
        "Now that we have the basics set up - let's see what we mean by \"Retrieval Augmented\" Generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7o8aXbhRAPe"
      },
      "source": [
        "## Naive RAG - Manually adding context through the Prompt Template\n",
        "\n",
        "Let's look at how our model performs at a simple task - defining what LangChain is!\n",
        "\n",
        "We'll redo some of our previous work to change the `system_template` to be less...verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu_Uox_pPKaf",
        "outputId": "5b93eef2-c844-4a23-cd87-94b1686a943a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It provides the necessary tools and components to streamline the creation of a wide range of applications, including question-answering systems, chatbots, and more. LangChain is optimized for integrating with various data sources, managing the flow of information, and enabling complex interactions with LLMs. By offering a structured approach, LangChain helps developers efficiently build and deploy sophisticated language model-driven solutions.' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 22, 'total_tokens': 118}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-bf9d11de-ca94-43e8-b878-60e257b7b2f7-0' usage_metadata={'input_tokens': 22, 'output_tokens': 96, 'total_tokens': 118}\n"
          ]
        }
      ],
      "source": [
        "system_template = \"You are a helpful assistant.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KXqGI4XbMb"
      },
      "source": [
        "Well, that's not very good - is it!\n",
        "\n",
        "The issue at play here is that our model was not trained on the idea of \"LangChain\", and so it's left with nothing but a guess - definitely not what we want the answer to be!\n",
        "\n",
        "Let's ask another simple LangChain question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRG5LwYoXnsr",
        "outputId": "58769a17-aba2-45f8-e9d4-23e96314de54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a specialized language designed to facilitate the interaction with language models in a structured and efficient manner. It is part of the LangChain framework, which aims to simplify the process of building applications that leverage the capabilities of language models. LCEL allows users to define and manipulate expressions that can be evaluated by language models, enabling more complex and dynamic interactions.\\n\\nLCEL is particularly useful for tasks that require chaining multiple language model operations together, managing state, and integrating with external systems or APIs. By using LCEL, developers can create more robust and maintainable code when working with language models, improving the overall efficiency and effectiveness of their applications.' response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 27, 'total_tokens': 161}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-fa7965ad-620d-4dcf-8dec-7f449bcef17a-0' usage_metadata={'input_tokens': 27, 'output_tokens': 134, 'total_tokens': 161}\n"
          ]
        }
      ],
      "source": [
        "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LCEL)?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63pr0fgYXxC3"
      },
      "source": [
        "While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!\n",
        "\n",
        "However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgr25HjgYHwh",
        "outputId": "7d590389-8a46-4105-fbe7-c08495dced25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It offers several benefits, including:\\n\\n- Full sync, async, batch, and streaming support, making it easy to prototype chains and expose them in different interfaces.\\n- Graceful error handling through easily attached fallbacks.\\n- Automatic parallelism for components that can be run in parallel.\\n- Seamless integration with LangSmith Tracing for maximal observability and debuggability of complex chains.' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 274, 'total_tokens': 370}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-56408c89-90ca-477c-b8a7-27db8f2a5e04-0' usage_metadata={'input_tokens': 274, 'output_tokens': 96, 'total_tokens': 370}\n"
          ]
        }
      ],
      "source": [
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provided context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, respond with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "CONTEXT = \"\"\"\n",
        "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
        "\n",
        "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQdtCedY7C4"
      },
      "source": [
        "You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!\n",
        "\n",
        "> NOTE: While RAG is an effective strategy to *help* ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes\n",
        "\n",
        "That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!\n",
        "\n",
        "However, manually providing context is not scalable - and doesn't really offer any benefit.\n",
        "\n",
        "Enter: Retrieval Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmdARsVBJUq"
      },
      "source": [
        "## Task #4: Implement Naive RAG using LCEL\n",
        "\n",
        "Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4AozVoEZveK"
      },
      "source": [
        "## Putting the R in RAG: Retrieval 101\n",
        "\n",
        "In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
        "\n",
        "Let's tackle an immediate problem first: The Context Window.\n",
        "\n",
        "All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
        "\n",
        "Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!\n",
        "\n",
        "> NOTE: It is recommended you do not run the following cells, they are purely for demonstrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PbXBxffibeyp"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "EVERY HITCHHIKER'S GUIDE BOOK\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvgFuaXcHFT"
      },
      "source": [
        "We can leverage our tokenizer to count the number of tokens for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HaKPOdSjbifn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDiSMxpE4Xi",
        "outputId": "886fd517-9128-45ca-9fbd-5152ae7f146b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(enc.encode(context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUuZpAicLdm"
      },
      "source": [
        "The full set comes in at a whopping *636,144* tokens.\n",
        "\n",
        "So, we have too much context. What can we do?\n",
        "\n",
        "Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even `gpt-4-32k` wouldn't be able to fit that whole text in the context window at once.\n",
        "\n",
        "So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
        "\n",
        "We have another problem now.\n",
        "\n",
        "If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
        "\n",
        "> NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
        "\n",
        "In order to conceptualize the following processes - let's create a toy context set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPCiOPwUfbqn"
      },
      "source": [
        "### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 100 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nLW9AfDKfVHn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nPYPBe2ngT9N"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_RGVlTihaQX",
        "outputId": "d1b260d2-8559-4ccc-d333-9e66543d9ed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTYny2xchS_Z",
        "outputId": "35a390f4-2e17-46f6-b4d2-7c9c86773507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "----\n",
            "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "----\n",
            "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for chunk in chunks:\n",
        "  print(chunk)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hOgu5Yhefv"
      },
      "source": [
        "As is shown in our result, we've split each section into 100 token chunks - cleanly separated by `\\n\\n` characters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTiJ2utMpqq"
      },
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "1. `By Page`\n",
        "2. `By Page Segment - Image, Table, etc.`\n",
        "3. `By Paragraph`\n",
        "4. `By Entire Document`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj18Rjafhp7d"
      },
      "source": [
        "## Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "We'll be using OpenAI's `text-embedding-3` model as our embedding model today!\n",
        "\n",
        "Let's load it up through LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "quNjOLWspOVN"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGZ92hm9IeX"
      },
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "What is the embedding dimension, given that we're using `text-embedding-3-small`?\n",
        "\n",
        "> HINT: Check out the [docs](https://platform.openai.com/docs/guides/embeddings) to help you answer this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  1536\n",
        "\n",
        "By default, the length of the embedding vector will be 1536 for text-embedding-3-small or 3072 for text-embedding-3-large. You can reduce the dimensions of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties. We go into more detail on embedding dimensions in the embedding use case section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByK-zb0FsnqR"
      },
      "source": [
        "### Finding the Embeddings for Our Chunks\n",
        "\n",
        "First, let's find all our embeddings for each chunk and store them in a convenient format for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZHl-u6Fxske9"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ9wY2etK7y",
        "outputId": "bc3554eb-9075-4a7f-c028-7dae06e7256c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for k,v in embeddings_dict.items():\n",
        "  print(f\"Chunk - {k}\")\n",
        "  print(\"---\")\n",
        "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOxYybdNtkWv"
      },
      "source": [
        "Okay, great. Let's create a query - and then embed it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQD5Zwl1tLrZ",
        "outputId": "d42d7488-edca-458b-dca4-1b11f3536cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector of Size: 1536\n"
          ]
        }
      ],
      "source": [
        "query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "query_vector = embedding_model.embed_query(query)\n",
        "print(f\"Vector of Size: {len(query_vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJhyvgEt5Vt"
      },
      "source": [
        "Now, let's compare it against each existing chunk's embedding by using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vfyDZlpmfWYa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwnJ0uYQt-G_",
        "outputId": "5ed2d13c-564c-4e2d-b06a-9f4e2e75d9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "0.5372469895308204\n"
          ]
        }
      ],
      "source": [
        "max_similarity = -float('inf')\n",
        "closest_chunk = \"\"\n",
        "\n",
        "for chunk, chunk_vector in embeddings_dict.items():\n",
        "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "  if cosine_similarity_score > max_similarity:\n",
        "    closest_chunk = chunk\n",
        "    max_similarity = cosine_similarity_score\n",
        "\n",
        "print(closest_chunk)\n",
        "print(max_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDC7HS7iumN-"
      },
      "source": [
        "And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPJexL1kuw45"
      },
      "source": [
        "### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "First, we'll wrap the above in a helper function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tnLpo26pu8-1"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query, embeddings_dict, embedding_model):\n",
        "  query_vector = embedding_model.embed_query(query)\n",
        "  max_similarity = -float('inf')\n",
        "  closest_chunk = \"\"\n",
        "\n",
        "  for chunk, chunk_vector in embeddings_dict.items():\n",
        "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "    if cosine_similarity_score > max_similarity:\n",
        "      closest_chunk = chunk\n",
        "      max_similarity = cosine_similarity_score\n",
        "\n",
        "  return closest_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrINkVrvL1Q"
      },
      "source": [
        "Now, let's add it to our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Q26pl1osvNmL"
      },
      "outputs": [],
      "source": [
        "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
        "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
        "\n",
        "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
        "\n",
        "  return_package = {\n",
        "      \"query\" : query,\n",
        "      \"response\" : response,\n",
        "      \"retriever_context\" : context\n",
        "  }\n",
        "\n",
        "  return return_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHjbWxTAvtLS",
        "outputId": "86e4baf5-a3e4-44c7-ba85-19f7a1d852ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Can LCEL help take code from the notebook to production?',\n",
              " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. Because any chain constructed with LCEL will automatically have full sync, async, batch, and streaming support, it allows for easy prototyping in a Jupyter notebook using the sync interface and then exposing it as an async streaming interface, facilitating a smooth transition from prototyping to production.', response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 152, 'total_tokens': 222}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-e9558e37-7185-429d-8c18-c0a310e92729-0', usage_metadata={'input_tokens': 152, 'output_tokens': 70, 'total_tokens': 222}),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2URVX3O3XJ"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "What does LCEL do that makes it more reliable at scale?\n",
        "\n",
        "> HINT: Use your newly created `simple_rag` to help you answer this question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  LCEL provides full support for sync, async, batch, and streaming interfaces.\n",
        "\n",
        "- allows for easier prototyping and transitioning to different execution models, which enhances reliability and scalability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "lcel_scaling = simple_rag(\"What does LCEL do that makes it more reliable at scale?\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What does LCEL do that makes it more reliable at scale?',\n",
              " 'response': AIMessage(content='LCEL makes chains more reliable at scale by providing automatic support for sync, async, batch, and streaming operations. This ensures that chains can be efficiently prototyped and then scaled up to handle different types of workloads and concurrency requirements without needing significant changes to the code.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 153, 'total_tokens': 207}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-0b4c6878-50e8-45f5-be45-e2e4f3ac29a0-0', usage_metadata={'input_tokens': 153, 'output_tokens': 54, 'total_tokens': 207}),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(lcel_scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_tokens': 153, 'output_tokens': 54, 'total_tokens': 207}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lcel_scaling[\"response\"].content\n",
        "lcel_scaling[\"response\"].response_metadata\n",
        "lcel_scaling[\"response\"].usage_metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfYsBu45-0pL"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #5: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and OpenAI to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using Drant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "Let's use [Steve Jobs iPhone 2007 Presentation Introduction Speech PDF](https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf) as our data today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `PyMUPDFLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-AHA9L3Jxo3r"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = PyMuPDFLoader(\"https://singjupost.com/wp-content/uploads/2014/07/Steve-Jobs-iPhone-2007-Presentation-Full-Transcript.pdf\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "4da462d2-e4db-4c89-b84c-2e4f85ae1c09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "43302516-c252-4b3b-a723-75be9728dfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "197\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Steve Job's Speech\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a prompt based on the following rules:  Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "Answer questions using only the provided context. If you do not know the answer, respond with \"I don't know\".  If the question is unrelated to the context, respond with \"ERROR!!  I cannot answer this question as it is unrelated to the context.\"\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\nCONTEXT:\\n{context}\\n\\nQUERY:\\n{question}\\n\\nAnswer questions using only the provided context. If you do not know the answer, respond with \"I don\\'t know\".  If the question is unrelated to the context, respond with \"ERROR!!  I cannot answer this question as it is unrelated to the context.\"\\n'))])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the LANGCHAIN_API_KEY environment variable (create key in settings)\n",
        "from langchain import hub\n",
        "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "rlm_prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the rlm ChatPromptTemplate from the hub\n",
        "rlm_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "1cbcd0a6-1bb6-497e-dbf1-6a0eb5f66353"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "60622d4c-87fd-4b02-ba18-ba2e1180efe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       +---------------------------------+                         \n",
            "                       | Parallel<context,question>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                           *****                   ****                            \n",
            "                        ***                            ****                        \n",
            "                     ***                                   ****                    \n",
            "+--------------------------------+                             **                  \n",
            "| Lambda(itemgetter('question')) |                              *                  \n",
            "+--------------------------------+                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "     | VectorStoreRetriever |                   | Lambda(itemgetter('question')) | \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "                           *****                   *****                           \n",
            "                                ***             ***                                \n",
            "                                   ***       ***                                   \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<context,question>Output |                        \n",
            "                       +----------------------------------+                        \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                            +------------------------+                             \n",
            "                            | Parallel<context>Input |                             \n",
            "                            +------------------------+                             \n",
            "                              ****               ****                              \n",
            "                           ***                       ***                           \n",
            "                         **                             **                         \n",
            "     +-------------------------------+              +-------------+                \n",
            "     | Lambda(itemgetter('context')) |              | Passthrough |                \n",
            "     +-------------------------------+              +-------------+                \n",
            "                              ****               ****                              \n",
            "                                  ***         ***                                  \n",
            "                                     **     **                                     \n",
            "                           +-------------------------+                             \n",
            "                           | Parallel<context>Output |                             \n",
            "                           +-------------------------+                             \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                       +---------------------------------+                         \n",
            "                       | Parallel<response,context>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                             ***                  ****                             \n",
            "                         ****                         ***                          \n",
            "                       **                                ****                      \n",
            "         +--------------------+                              **                    \n",
            "         | ChatPromptTemplate |                               *                    \n",
            "         +--------------------+                               *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "             +------------+                  +-------------------------------+     \n",
            "             | ChatOpenAI |                  | Lambda(itemgetter('context')) |     \n",
            "             +------------+**                +-------------------------------+     \n",
            "                             ***                  ***                              \n",
            "                                ****          ****                                 \n",
            "                                    **      **                                     \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<response,context>Output |                        \n",
            "                       +----------------------------------+                        \n",
            "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
            "graph TD;\n",
            "\tParallel_context_question_Input[Parallel_context_question_Input]:::startclass;\n",
            "\tParallel_context_question_Output([Parallel<context,question>Output]):::otherclass;\n",
            "\tLambda_itemgetter__question___([Lambda(itemgetter('question'))]):::otherclass;\n",
            "\tVectorStoreRetriever([VectorStoreRetriever]):::otherclass;\n",
            "\tLambda_itemgetter__question___([Lambda(itemgetter('question'))]):::otherclass;\n",
            "\tParallel_context_Input([Parallel<context>Input]):::otherclass;\n",
            "\tParallel_context_Output([Parallel<context>Output]):::otherclass;\n",
            "\tLambda_itemgetter__context___([Lambda(itemgetter('context'))]):::otherclass;\n",
            "\tPassthrough([Passthrough]):::otherclass;\n",
            "\tParallel_response_context_Input([Parallel<response,context>Input]):::otherclass;\n",
            "\tParallel_response_context_Output[Parallel_response_context_Output]:::endclass;\n",
            "\tChatPromptTemplate([ChatPromptTemplate]):::otherclass;\n",
            "\tChatOpenAI([ChatOpenAI]):::otherclass;\n",
            "\tLambda_itemgetter__context___([Lambda(itemgetter('context'))]):::otherclass;\n",
            "\tLambda_itemgetter__question___ --> VectorStoreRetriever;\n",
            "\tParallel_context_question_Input --> Lambda_itemgetter__question___;\n",
            "\tVectorStoreRetriever --> Parallel_context_question_Output;\n",
            "\tParallel_context_question_Input --> Lambda_itemgetter__question___;\n",
            "\tLambda_itemgetter__question___ --> Parallel_context_question_Output;\n",
            "\tParallel_context_Input --> Lambda_itemgetter__context___;\n",
            "\tLambda_itemgetter__context___ --> Parallel_context_Output;\n",
            "\tParallel_context_Input --> Passthrough;\n",
            "\tPassthrough --> Parallel_context_Output;\n",
            "\tParallel_context_question_Output --> Parallel_context_Input;\n",
            "\tChatPromptTemplate --> ChatOpenAI;\n",
            "\tParallel_response_context_Input --> ChatPromptTemplate;\n",
            "\tChatOpenAI --> Parallel_response_context_Output;\n",
            "\tParallel_response_context_Input --> Lambda_itemgetter__context___;\n",
            "\tLambda_itemgetter__context___ --> Parallel_response_context_Output;\n",
            "\tParallel_context_Output --> Parallel_response_context_Input;\n",
            "\tclassDef startclass fill:#ffdfba;\n",
            "\tclassDef endclass fill:#baffc9;\n",
            "\tclassDef otherclass fill:#fad7de;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())\n",
        "print(retrieval_augmented_qa_chain.get_graph().draw_mermaid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to render the graph using the Mermaid.INK API. Status code: 400.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mermaid diagramming does not appear to work yet...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mretrieval_augmented_qa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/aie3-bootcamp/AIE3/Week 3/Day 2/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph.py:454\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_mermaid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    449\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    450\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[1;32m    451\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[1;32m    452\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    453\u001b[0m )\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/aie3-bootcamp/AIE3/Week 3/Day 2/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:156\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[0m\n\u001b[1;32m    150\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    151\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    152\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 156\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
            "File \u001b[0;32m~/aie3-bootcamp/AIE3/Week 3/Day 2/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:279\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_bytes\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to render the graph using the Mermaid.INK API. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to render the graph using the Mermaid.INK API. Status code: 400."
          ]
        }
      ],
      "source": [
        "# Mermaid diagramming does not appear to work yet...\n",
        "print(retrieval_augmented_qa_chain.get_graph().draw_mermaid_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retrieval_augmented_qa_chain.get_graph().draw_mermaid_png()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the most important thing about the iPhone?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "cd2c86ee-ca37-481b-f2d2-2fd564855cd8"
      },
      "outputs": [],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSZFdCM5LFoq",
        "outputId": "4121ebc7-3a23-4501-8984-5c7f7b81b7a9"
      },
      "outputs": [],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "a5082ad5-4fb6-4938-f9d8-8fa606622bfd"
      },
      "outputs": [],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvkSiGXrPMYw"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What key innovations did the iPhone introduce?\n",
        "\n",
        "> HINT: Use your RAG Chain to answer this question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ANSWER:  The key innovations the iPhone introduced include:\n",
        "\n",
        "1. **Widescreen iPod with Touch Controls**: The iPhone combined the functionality of an iPod with a widescreen and touch controls, allowing users to interact with their music in a new way.\\n   \\n2. **Revolutionary Mobile Phone**: The iPhone aimed to reinvent the phone by making it easier to make calls, sync contacts from a PC or Mac, and use those contacts more efficiently.\n",
        "3. **Breakthrough Internet Communications Device**: The iPhone also served as an advanced internet communicator.\n",
        "4. **Multi-Touch Screen**: The iPhone featured a multi-touch screen, a first in mobile devices.\n",
        "5. **Miniaturization and Custom Silicon**: It showcased advanced miniaturization and the use of custom silicon.\n",
        "6. **Tremendous Power Management**: The iPhone had advanced power management capabilities.\n",
        "7. **OSX Inside a Mobile Device**: The iPhone incorporated OSX, bringing desktop-class applications to a mobile device.\n",
        "8. **Featherweight Precision Enclosures and Advanced Sensors**: It featured lightweight, precise enclosures and included three advanced sensors.\n",
        "9. **Desktop Class Applications and Widescreen Video iPod**: The iPhone supported desktop-class applications and acted as a widescreen video iPod.\n",
        "10. **Over 200 Patents**: Apple filed for over 200 patents for the various inventions included in the iPhone\n",
        "\n",
        "These innovations collectively positioned the iPhone as a groundbreaking device that combined multiple functionalities into a single, compact form factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What key innovations did the iPhone introduce?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response[\"response\"].content"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
