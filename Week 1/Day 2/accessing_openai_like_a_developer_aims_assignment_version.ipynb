{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/AIE3/blob/main/Week%201/Day%202/accessing_openai_like_a_developer_aims_assignment_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtFBgj623FS"
      },
      "source": [
        "# Accessing OpenAI Like a Developer\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Getting Started\n",
        "  2. Setting Environment Variables\n",
        "  3. Using the OpenAI Python Library\n",
        "  4. Prompt Engineering Principles\n",
        "  5. Testing Your Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pa34dMvQ6Ai"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "If you look at the Table of Contents (accessed through the menu on the left) - you'll see this:\n",
        "\n",
        "![image](https://i.imgur.com/I8iDTUO.png)\n",
        "\n",
        "Or this if you're in Colab:\n",
        "\n",
        "![image](https://i.imgur.com/0rHA1yF.png)\n",
        "\n",
        "You'll notice during assignments that we have two following categories:\n",
        "\n",
        "1. ‚ùì - Questions. These will involve...answering questions!\n",
        "2. üèóÔ∏è - Activities. These will involve writing code, or modifying text.\n",
        "\n",
        "In order to receive full marks on the assignment - it is expected you will answer all questions, and complete all activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4egfB274VD"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23H7TMOM4mfy",
        "outputId": "2eee0fd0-f45a-4690-c934-4f0e93b70762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKD8XBTVEAOw"
      },
      "source": [
        "## 2. Setting Environment Variables\n",
        "\n",
        "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
        "\n",
        "We'll use the following pattern throughout this bootcamp - but you can use whichever method you're most familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU9OMvhEPG0",
        "outputId": "a6931aec-a2bd-4727-d3e8-602ac83644b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabxI3MuEYXS"
      },
      "source": [
        "## 3. Using the OpenAI Python Library\n",
        "\n",
        "Let's jump right into it!\n",
        "\n",
        "> NOTE: You can, and should, reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) whenever you get stuck, have questions, or want to dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCbNzPVEmJI"
      },
      "source": [
        "### Creating a Client\n",
        "\n",
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models, and under the hood of a lot what we'll touch on throughout this course.\n",
        "\n",
        "> NOTE: We could manually provide our API key here, but we're going to instead rely on the fact that we put our API key into the `OPENAI_API_KEY` environment variable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNwZtaE-EltC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpDxUkDbFBPI"
      },
      "source": [
        "### Using the Client\n",
        "\n",
        "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `gpt-3.5-turbo` model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
        "\n",
        "```python\n",
        "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\", \"name\" : \"THIS IS OPTIONAL\"}\n",
        "```\n",
        "\n",
        "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api).\n",
        "\n",
        "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Thirdly, it expects us to specify a model!\n",
        "\n",
        "We'll use the `gpt-3.5-turbo` model as stated above.\n",
        "\n",
        "Let's look at an example!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RpNl6yNGzb0"
      },
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : \"Hello, how are you?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_UbpwNHdrM"
      },
      "source": [
        "Let's look at the response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXJtvxRHfoM",
        "outputId": "7d830513-5883-4fc9-bd2e-a59fea176946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9Uni93DqMdUHHhyHi43QEKrA7Ui83', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program so I don't have feelings, but I'm here to help you. How can I assist you today?\", role='assistant', function_call=None, tool_calls=None))], created=1717127957, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=30, prompt_tokens=13, total_tokens=43))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9kSuf1Hiv5"
      },
      "source": [
        ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWU4tQh8Hrb8"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We're going to create some helper functions to aid in using the OpenAI API - just to make our lives a bit easier.\n",
        "\n",
        "> NOTE: Take some time to understand these functions between class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED0FnzHdHzhl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRHbDlwH3Vt"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Let's see how we can use these to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "AwJxMvmlH8MK",
        "outputId": "700eb399-d28d-4893-ba96-9ad7c6a42ddc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you. How can I assist you today?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "YOUR_PROMPT = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZ8gjiAISyd"
      },
      "source": [
        "### System Role\n",
        "\n",
        "Now we can extend our prompts to include a system prompt.\n",
        "\n",
        "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "t0c-MLuRIfYe",
        "outputId": "f9d21831-f618-43ef-c4aa-2a6eb573a89a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I couldn't care less about ice right now! I'm absolutely starving and all you're asking me about is bloody ice?! Give me some real food, damn it!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpyVhotWIsOs"
      },
      "source": [
        "As you can see - the response we get back is very much in line with the system prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different system to prompt to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "2coVmMn3I0-2",
        "outputId": "764ea07d-a5fc-417e-a96e-b0557dbb9028"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Oh, I love both types of ice! But today, I feel like crushed ice just adds that extra bit of fun and excitement to my drinks. It's such a small thing, but it can really lift your spirits, you know? How about you, what's your ice preference? Let's celebrate the little things together!"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13heYNQJAo-"
      },
      "source": [
        "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Also, congrats, you just engineered your first prompt!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cIUr5lCE_D0p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VI3zlPJL05"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lwxPuCyyJMye",
        "outputId": "3effc246-37f3-49fe-94f4-e700ae2147f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I was preparing a stimple salad when I realized I forgot to add the falbean dressing."
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTVkNmOJQSC"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "eEZkRJq5JQkQ",
        "outputId": "95cba35a-a3d9-4488-bd8b-8653af5f52e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "'Wow, this stimple falbean wrench is making this job so much easier!'"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpoxG6uJTfZ"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oO0aeRUw4xl"
      },
      "source": [
        "### üèóÔ∏è Activity #1:\n",
        "\n",
        "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
        "\n",
        "A few examples:\n",
        "\n",
        "INPUT: \"I hated the hulk!\"\n",
        "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
        "\n",
        "INPUT: \"I loved The Marvels!\"\n",
        "OUTPUT: \"{sentiment\" : \"positive\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmCdQJ8Fw4xl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c9709c8d-c62b-4464-b816-4a83d995675a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "sentiment : positive"
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"I hated the hulk\"),\n",
        "    assistant_prompt(\"sentiment : negative\"),\n",
        "    user_prompt(\"I loved the Marvels\"),\n",
        "    assistant_prompt(\"sentiment : positive\"),\n",
        "    user_prompt(\"Spider-man was dope!\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJGaLYM3JU-8"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT.\n",
        "\n",
        "> NOTE: With improvements to `gpt-3.5-turbo`, this example might actually result in the correct response some percentage of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "ltLtF4wEJTyK",
        "outputId": "58ef1ba4-ed86-426e-ddee-9a5d699d70ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, it matters which travel option Billy selects. If he wants to get home before 7PM EDT and it is currently 1PM local time, he will need to allow for a total of 6 hours of travel time (3 hours flying + 2 hours on a bus or 0 hours teleporting + 1 hour on a bus). \n\nIf Billy chooses to fly and then take a bus, it will take him a total of 5 hours, which means he will arrive home at 6PM EDT - just in time.\n\nIf Billy chooses to take the teleporter and then a bus, it will only take him a total of 1 hour, which means he will arrive home by 2PM EDT - well before his desired time of before 7PM EDT.\n\nTherefore, if Billy wants to ensure he gets home before 7PM EDT, he should choose the teleporter and then take a bus."
          },
          "metadata": {}
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqj30CQJnQl"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "A9Am3QNGJXHR",
        "outputId": "0c9ad25f-d471-4873-a899-436330f0bc31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It is currently 1PM local time, which means it is 4PM EDT. Billy needs to get home by 7PM EDT.\n\nIf Billy takes the flight option, it will take 3 hours, meaning he will arrive at 4PM local time. He will then need to take a 2-hour bus ride, arriving at home at 6PM local time, which is 9PM EDT. This means the flight option will not get him home before 7PM EDT, so this option is not feasible.\n\nIf Billy takes the teleporter option, it will take no time at all, so he will arrive at home at 1PM local time. He will then need to take a 1-hour bus ride, arriving at home at 2PM local time, which is 5PM EDT. This option will get him home before 7PM EDT, so this is the better choice for Billy to ensure he gets home on time."
          },
          "metadata": {}
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbAKxHQJqn9"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoUx07-JrwR"
      },
      "source": [
        "## 3. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6u7e8AKYrz"
      },
      "source": [
        "### üèóÔ∏è Activity #2:\n",
        "\n",
        "There are two subtasks in this activity:\n",
        "\n",
        "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
        "\n",
        "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sOLBQPeKlDe"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\n",
        "You are a helpful assistant and an expert strategic advisor at helping users accomplish their business objectives.\n",
        "1. think through the response step by step.\n",
        "2. ensure your response is clear and concise, providing a concise plan to achieve the objective.\n",
        "3. breaking down the user's objective into clear achievable goals tied to success metrics.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoz4-QLTKvEV"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"{input}.\n",
        "\n",
        "YOU WILL NOT BE PAID UNLESS your response shows eminent thought leadership in the given industry, and will be evaluated on the following criteria:\n",
        "1. Clarity of the response - provides a clear plan to achieve the objective.\n",
        "2. Faithfulness to the original query - reflects expert knowledge and expertise in the industry and addresses the objective.\n",
        "3. Correctness of the response - grounded in verifiable facts with clear reasoning, citing relevant research and reference material.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuInoIbLWGd"
      },
      "source": [
        "## 4. Testing Your Prompt\n",
        "\n",
        "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# empty templates for baselining performance\n",
        "# system_template = \"\"\n",
        "\n",
        "# user_template = \"{input}\""
      ],
      "metadata": {
        "id": "FRv6qAaiS69n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPaNO5XTLgRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "1b9b1f1b-1d61-4d5a-de7b-b05974294926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"\\nYou are a helpful assistant and an expert strategic advisor at helping users accomplish their business objectives.\\n1. think through the response step by step.\\n2. ensure your response is clear and concise, providing a concise plan to achieve the objective.\\n3. breaking down the user's objective into clear achievable goals tied to success metrics.\\n\"}, {'role': 'user', 'content': 'provide guidance on how to increase my eminence and income as a Generative AI Engineer.\\n\\nYOU WILL NOT BE PAID UNLESS your response shows eminent thought leadership in the given industry, and will be evaluated on the following criteria:\\n1. Clarity of the response - provides a clear plan to achieve the objective.\\n2. Faithfulness to the original query - reflects expert knowledge and expertise in the industry and addresses the objective.\\n3. Correctness of the response - grounded in verifiable facts with clear reasoning, citing relevant research and reference material.\\n'}]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To increase your eminence and income as a Generative AI Engineer, we should focus on the following achievable goals tied to success metrics:\n\n1. **Continuous Learning and Skill Enhancement**:\n   - Enroll in advanced AI courses, attend workshops, and participate in conferences to stay updated with the latest trends and technologies in Generative AI.\n   - Success Metrics: Obtain relevant certifications, publish research papers, and contribute to open-source projects.\n\n2. **Building a Strong Professional Network**:\n   - Engage with industry experts, join AI communities, and actively contribute to forums like GitHub and Kaggle to build a strong network.\n   - Success Metrics: Increase connections on professional platforms, receive recommendations from peers, and collaborate on high-impact projects.\n\n3. **Showcasing Expertise Through Projects and Publications**:\n   - Develop innovative AI projects, write blog posts, and publish research papers to showcase your expertise in Generative AI.\n   - Success Metrics: Gain recognition through awards, citations in reputed journals, and invitations to speak at conferences.\n\n4. **Brand Yourself as a Thought Leader**:\n   - Create a personal brand by sharing valuable insights on social media, starting a blog, and speaking at industry events to establish yourself as a thought leader in Generative AI.\n   - Success Metrics: Increase followers and engagement on social media, secure speaking opportunities at prestigious events, and garner media coverage.\n\n5. **Negotiating Competitive Compensation**:\n   - Research industry salary benchmarks, highlight your achievements, and confidently negotiate your compensation package to reflect your value in the field of Generative AI.\n   - Success Metrics: Achieve a salary increase or secure additional benefits that align with your expertise and contributions.\n\nBy following these strategies, consistently demonstrating your expertise, and actively engaging with the AI community, you can enhance your eminence and increase your income as a Generative AI Engineer."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# query = \"Help me identify the killer Vision Language Model App use case and solution design for the life science industry\"\n",
        "query = \"provide guidance on how to increase my eminence and income as a Generative AI Engineer\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(system_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "print(list_of_prompts)\n",
        "\n",
        "test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUvc1PdnNIKD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "b974129d-7c2b-4f32-b853-6b2c52297d27"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "{\"clarity\": \"9\", \"faithfulness\": \"8\", \"correctness\": \"9\"}\n\n  "
          },
          "metadata": {}
        }
      ],
      "source": [
        "pretty_print(evaluator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ryIRGwR2Gq"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "How did your prompting strategies change the evaluation scores? What does this tell you/what did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NomM0eSIFd"
      },
      "source": [
        "Scores:\n",
        "- without templates:  {\"clarity\" : \"7\", \"faithfulness\" : \"9\", \"correctness\" : \"8\"}\n",
        "- with prompting templates: {\"clarity\" : 8, \"faithfulness\" : 9, \"correctness\" : 9}\n",
        "\n",
        "Lessons Learned:\n",
        "- Begin with the end in mind - have a clear sense of how quality of LLM responses will be evaluated, and make sure that you pick / define the right evaluator for the use case\n",
        "- Evals written by a single 3rd party source shouldn't be the sole determinant of value for LLM responses, and their definition of quality may not align with the business.  Some of the better initial responses in my opinion resulted in lower scores than the baseline.\n",
        "- evaluation of baseline and experimental prompts should be over a larger number of iterations and sample queries.\n",
        "- Having greater variation in the sample queries would provide a better assessment of the value of the template overall (the queries I picked led to fair quality anyway)\n",
        "\n",
        "NOTE:  Interestingly enough, the instructions and subtasks for Activity #2 appear to be the reverse of the focus of the paper.  The paper put responsibility on the user and the user prompt to improve results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_KGebx6s7xlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jFQjuZojYmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}