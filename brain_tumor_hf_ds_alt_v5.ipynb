{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/AIE3/blob/main/brain_tumor_hf_ds_alt_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuBoZd6_Kw7s"
      },
      "source": [
        "# Brain Tumor Image Dataset - Hugging Face Dataset Creation\n",
        "\n",
        "This notebook processes a brain tumor image dataset with COCO-format annotations and creates a Hugging Face Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key learnings and decisions while creating this brain tumor image dataset for Hugging Face.\n",
        "\n",
        "1. Data Structure:\n",
        "   - The dataset contains brain tumor images with COCO-format annotations.\n",
        "   - Annotations include bounding boxes and polygon segmentations.\n",
        "   - Categories are structured as: ID 0 (Tumor), ID 1 (0), ID 2 (1), with 1 and 2 being subcategories of Tumor.\n",
        "\n",
        "2. Segmentation Characteristics:\n",
        "   - Segmentations are not rectangular but complex polygons.\n",
        "   - 100% of segmentations are near their bounding boxes.\n",
        "   - 100% of sampled annotations have valid polygon segmentations.\n",
        "\n",
        "3. Key Adjustments:\n",
        "   - We adjusted the 'features' definition to accommodate complex polygon segmentations.\n",
        "   - The ClassLabel for 'category_id' was updated to ['Tumor', '0', '1'] to match the actual data structure.\n",
        "\n",
        "4. Data Loading Considerations:\n",
        "   - Image data is stored as bytes after being read with OpenCV.\n",
        "   - Segmentation data needs to be carefully handled to maintain its list-of-lists structure.\n",
        "\n",
        "5. Verification Steps:\n",
        "   - We implemented functions to verify polygon validity and dataset structure.\n",
        "   - These checks are crucial before pushing the dataset to the Hugging Face Hub.\n",
        "\n",
        "6. Performance and Efficiency:\n",
        "   - We used tqdm for progress tracking during image loading, which is helpful for large datasets.\n",
        "   - The dataset creation process involves reading and encoding many images, which can be time-consuming.\n",
        "\n",
        "7. Hugging Face Dataset Structure:\n",
        "   - The dataset is created using the Hugging Face Datasets library, which has specific requirements for data types and structures.\n",
        "   - We needed to ensure that all data types in the pandas DataFrame matched the defined features.\n",
        "\n",
        "8. Potential Future Work:\n",
        "   - The current implementation doesn't push to the Hugging Face Hub automatically. This step should be done manually after thorough verification.\n",
        "   - Depending on the specific use case, additional preprocessing or data augmentation steps might be necessary."
      ],
      "metadata": {
        "id": "1A7rmeq2T-1I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhhJ8h1uKw7t"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "This section imports necessary libraries and sets up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OQLXzGadLOnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pyarrow==14.0.1 requests==2.31.0"
      ],
      "metadata": {
        "id": "_ko8sN4vVrxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets==2.11.0"
      ],
      "metadata": {
        "id": "Hgjmz2RkVxVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, ClassLabel, Value, Sequence, Image\n",
        "from tqdm.auto import tqdm\n",
        "import cv2"
      ],
      "metadata": {
        "id": "22AUgBXjT3fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Constants and Configuration\n",
        "Define constants and configurations used throughout the notebook.\n"
      ],
      "metadata": {
        "id": "I7X6iGqZVBa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_DATASET_NAME = 'brain-tumor-image-dataset-semantic-segmentation-alt'\n",
        "SOURCE_JSON = \"/content/drive/MyDrive/kaggle/datasets/brain-tumor-image-dataset-semantic-segmentation/train/_annotations.coco.json\"\n",
        "SOURCE_IMAGE_DIR = \"/content/drive/MyDrive/kaggle/datasets/brain-tumor-image-dataset-semantic-segmentation/train\"\n"
      ],
      "metadata": {
        "id": "Wmu5BpQJbwqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FsovZqvKw7u"
      },
      "source": [
        "## 3. Feature Definition\n",
        "Define the structure of the Hugging Face Dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = Features({\n",
        "    'file_name': Value(dtype='string'),\n",
        "    'image': Value(dtype='string'),\n",
        "    'id': Value(dtype='int64'),\n",
        "    'category_id': ClassLabel(names=['0', '1']),  # Correct class labels\n",
        "    'bbox': Sequence(feature=Value(dtype='float32'), length=4),\n",
        "    'segmentation': Sequence(Sequence(Value(dtype='float32'))),\n",
        "    'area': Value(dtype='float32'),\n",
        "    'iscrowd': Value(dtype='int64'),\n",
        "    'height': Value(dtype='int64'),\n",
        "    'width': Value(dtype='int64'),\n",
        "    'date_captured': Value(dtype='string'),\n",
        "    'license': Value(dtype='int64')\n",
        "})"
      ],
      "metadata": {
        "id": "a00QTeM8ZGAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Loading and Preprocessing\n",
        "Functions to load and preprocess the COCO-format data."
      ],
      "metadata": {
        "id": "uvw_7Nn2VXbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZL8oddLKw7u"
      },
      "outputs": [],
      "source": [
        "def verify_source_data():\n",
        "    with open(SOURCE_JSON, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(\"Categories:\")\n",
        "    for category in data['categories']:\n",
        "        print(f\"ID: {category['id']}, Name: {category['name']}, Supercategory: {category['supercategory']}\")\n",
        "\n",
        "    category_counts = pd.DataFrame(data['annotations'])['category_id'].value_counts().sort_index()\n",
        "    print(\"\\nCategory distribution in annotations:\")\n",
        "    print(category_counts)\n",
        "\n",
        "    # Check for images with multiple bounding boxes\n",
        "    image_bbox_counts = pd.DataFrame(data['annotations'])['image_id'].value_counts()\n",
        "    print(f\"\\nImages with multiple bounding boxes: {(image_bbox_counts > 1).sum()}\")\n",
        "    print(f\"Max bounding boxes in an image: {image_bbox_counts.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_to_df():\n",
        "    with open(SOURCE_JSON, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create a dictionary to map category names to their IDs\n",
        "    category_name_to_id = {cat['name']: cat['id'] for cat in data['categories']}\n",
        "\n",
        "    images = pd.DataFrame(data['images'])\n",
        "    annotations = pd.DataFrame(data['annotations'])\n",
        "\n",
        "    df = pd.merge(images, annotations, left_on='id', right_on='image_id', suffixes=('', '_ann'))\n",
        "    df = df.drop(columns=['id_ann', 'image_id'])\n",
        "\n",
        "    # Store relative paths\n",
        "    df['image'] = df['file_name']\n",
        "\n",
        "    # Map category names to IDs in the DataFrame\n",
        "    df['category_id'] = df['category_id'].map(category_name_to_id)\n",
        "\n",
        "    # Inspect category_id values (for verification)\n",
        "    print(\"Unique category_id values:\", df['category_id'].unique())\n",
        "    print(\"Category_id value counts:\\n\", df['category_id'].value_counts())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "iCYaNVX3dN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def prepare_dataset_for_upload(dataset, hf_dataset_name):\n",
        "    # Create dataset directory\n",
        "    os.makedirs(hf_dataset_name, exist_ok=True)\n",
        "    os.makedirs(os.path.join(hf_dataset_name, \"images\"), exist_ok=True)\n",
        "\n",
        "    # Copy image files\n",
        "    for example in dataset:\n",
        "        src_path = os.path.join(SOURCE_IMAGE_DIR, example['file_name'])\n",
        "        dst_path = os.path.join(hf_dataset_name, \"images\", example['file_name'])\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    # Update image paths in the dataset\n",
        "    def update_image_path(example):\n",
        "        example['image'] = os.path.join(\"images\", example['file_name'])\n",
        "        return example\n",
        "\n",
        "    updated_dataset = dataset.map(update_image_path)\n",
        "\n",
        "    # Save the updated dataset\n",
        "    updated_dataset.save_to_disk(hf_dataset_name)\n",
        "\n",
        "    print(f\"Dataset prepared for upload in directory: {hf_dataset_name}\")\n",
        "\n",
        "# After creating and verifying the dataset\n",
        "prepare_dataset_for_upload(dataset, HF_DATASET_NAME)\n",
        "\n",
        "# Upload to Hugging Face Hub\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=HF_DATASET_NAME,\n",
        "    repo_id=f\"dwb2023/{HF_DATASET_NAME}\",\n",
        "    repo_type=\"dataset\"\n",
        ")"
      ],
      "metadata": {
        "id": "sO-7TtOdRzy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Dataset Creation\n",
        "Function to create the Hugging Face Dataset from the preprocessed data."
      ],
      "metadata": {
        "id": "BQ9sogWWV7KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as PILImage\n",
        "\n",
        "def create_hf_dataset(df, hf_dataset_name):\n",
        "    \"\"\"Creates a Hugging Face Dataset from preprocessed data.\"\"\"\n",
        "    # Ensure datatypes match the features\n",
        "    df['bbox'] = df['bbox'].apply(lambda x: [float(i) for i in x])\n",
        "    df['segmentation'] = df['segmentation'].apply(lambda x: [[float(i) for i in poly] for poly in x])\n",
        "    df['area'] = df['area'].astype('float32')\n",
        "\n",
        "    # Create the dataset\n",
        "    dataset = Dataset.from_pandas(df, features=features)\n",
        "    print(f\"Dataset created successfully with {len(dataset)} examples.\")\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VkLvwAuFmZkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Verification Functions\n",
        "Functions to verify the integrity and structure of the data and created dataset."
      ],
      "metadata": {
        "id": "C4jcZp4kWNhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "with open(SOURCE_JSON, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "category_ids = [ann['category_id'] for ann in coco_data['annotations']]\n",
        "id_counts = Counter(category_ids)\n",
        "\n",
        "categories = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
        "\n",
        "print(\"Category ID counts:\", id_counts)\n",
        "print(\"Category mappings:\", categories)\n",
        "\n",
        "# Automated verification\n",
        "expected_categories = {0: 'Tumor', 1: '0', 2: '1'}\n",
        "assert categories == expected_categories, f\"Category mismatch. Expected {expected_categories}, got {categories}\"\n",
        "assert set(id_counts.keys()) == {1, 2}, f\"Unexpected category IDs found: {set(id_counts.keys())}\""
      ],
      "metadata": {
        "id": "vY56XeTb6qZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_polygon(segmentation):\n",
        "    # Check if it's a list of lists\n",
        "    if not isinstance(segmentation, list) or not all(isinstance(poly, list) for poly in segmentation):\n",
        "        return False\n",
        "\n",
        "    # Check if each polygon has at least 6 coordinates (3 points)\n",
        "    if not all(len(poly) >= 6 and len(poly) % 2 == 0 for poly in segmentation):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "sample_annotations = coco_data['annotations'][:100]\n",
        "valid_count = sum(is_valid_polygon(ann['segmentation']) for ann in sample_annotations)\n",
        "valid_percentage = (valid_count / len(sample_annotations)) * 100\n",
        "print(f\"{valid_percentage:.2f}% of sampled annotations have valid polygon segmentations\")"
      ],
      "metadata": {
        "id": "AjQKcEaSMHGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_dataset(dataset):\n",
        "    print(f\"Dataset contains {len(dataset)} examples.\")\n",
        "    print(\"Sample of the first example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Check if all required fields are present\n",
        "    required_fields = ['file_name', 'image', 'id', 'category_id', 'bbox', 'segmentation', 'area']\n",
        "    for field in required_fields:\n",
        "        if field not in dataset[0]:\n",
        "            print(f\"Warning: '{field}' is missing from the dataset.\")\n",
        "\n",
        "    # Verify image data\n",
        "    if isinstance(dataset[0]['image'], str):\n",
        "        print(\"Image data is stored as file paths.\")\n",
        "    else:\n",
        "        print(\"Warning: Image data is not stored as file paths.\")\n",
        "\n",
        "    # Verify segmentation data\n",
        "    if isinstance(dataset[0]['segmentation'], list) and isinstance(dataset[0]['segmentation'][0], list):\n",
        "        print(\"Segmentation data is stored as a list of lists.\")\n",
        "    else:\n",
        "        print(\"Warning: Segmentation data is not stored as a list of lists.\")\n",
        "\n",
        "    print(\"Dataset verification complete.\")"
      ],
      "metadata": {
        "id": "DzgHdUtqOLlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Main Execution\n",
        "The main workflow to create and verify the dataset."
      ],
      "metadata": {
        "id": "D38qHafRW1lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "df = load_data_to_df()\n",
        "\n",
        "# Create the dataset\n",
        "dataset = create_hf_dataset(df, HF_DATASET_NAME)\n",
        "\n",
        "# Verify the dataset\n",
        "verify_dataset(dataset)\n",
        "\n",
        "# Prepare the dataset for upload\n",
        "prepared_dataset = prepare_dataset_for_upload(dataset, HF_DATASET_NAME)\n",
        "\n",
        "# Visualize samples from the prepared dataset\n",
        "visualize_samples(prepared_dataset)\n",
        "\n",
        "# Save the prepared dataset\n",
        "prepared_dataset.save_to_disk(HF_DATASET_NAME)\n",
        "\n",
        "print(f\"Dataset saved to {HF_DATASET_NAME}\")\n",
        "\n",
        "# Optional: Push to Hugging Face Hub\n",
        "# Uncomment the following lines when ready to upload\n",
        "# from huggingface_hub import HfApi\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=HF_DATASET_NAME,\n",
        "#     repo_id=f\"dwb2023/{HF_DATASET_NAME}\",\n",
        "#     repo_type=\"dataset\"\n",
        "# )\n",
        "# print(f\"Dataset uploaded to https://huggingface.co/datasets/dwb2023/{HF_DATASET_NAME}\")"
      ],
      "metadata": {
        "id": "pdF4aJRz92bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "PodhNCpXQFM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def visualize_samples(dataset, num_samples=5):\n",
        "    \"\"\"Visualize a few sample images with annotations.\"\"\"\n",
        "    for i in range(num_samples):\n",
        "        example = dataset[i]\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(os.path.join(SOURCE_IMAGE_DIR, example['file_name']))\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(img)\n",
        "\n",
        "        # Draw bounding box\n",
        "        bbox = example['bbox']\n",
        "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                             fill=False, edgecolor='red', linewidth=2)\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "        # Draw segmentation\n",
        "        seg = np.array(example['segmentation']).reshape(-1, 2)\n",
        "        plt.plot(seg[:, 0], seg[:, 1], color='blue', linewidth=2)\n",
        "\n",
        "        plt.title(f\"Category: {example['category_id']}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Call this function after dataset creation\n",
        "visualize_samples(dataset)"
      ],
      "metadata": {
        "id": "X7s8VdP6-z_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. (Optional) Upload to Hugging Face Hub\n",
        "Uncomment this section when ready to upload the dataset to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "u-UcN5SKTwA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Push to Hugging Face Hub\n",
        "dataset.push_to_hub(HF_DATASET_NAME)"
      ],
      "metadata": {
        "id": "5XnrOC47XAN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWdSrp_qQi9E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}